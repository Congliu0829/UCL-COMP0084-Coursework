{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwMS0_5Sbeyd",
        "outputId": "506caa8a-3cc4-4b11-ae8e-514a48360422"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "import transformers\n",
        "from transformers import BertTokenizerFast, BertModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QO-8pQGjW_0h",
        "outputId": "7965a144-2194-41f3-9c73-404581104334"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882: DtypeWarning: Columns (0,1,4) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm \n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "col=['qid','pid','query','passage','relevancy']\n",
        "\n",
        "train_data=pd.read_csv(\"part2/train_data.tsv\", sep='\\t', header=None, names=col)\n",
        "train_data=pd.DataFrame(train_data)\n",
        "train_data = train_data.iloc[1:]\n",
        "\n",
        "train_passages = train_data['passage'].values\n",
        "train_queries = train_data['query'].values\n",
        "train_pids = train_data['pid'].values.astype(np.int64)\n",
        "train_qids = train_data['qid'].values.astype(np.int64)\n",
        "train_labels = train_data['relevancy'].values.astype(np.float64).astype(np.int64)\n",
        "\n",
        "\n",
        "\n",
        "test_data=pd.read_csv(\"part2/validation_data.tsv\", sep='\\t', header=None, names=col)\n",
        "test_data=pd.DataFrame(test_data)\n",
        "test_data = test_data.iloc[1:]\n",
        "\n",
        "test_passages = test_data['passage'].values\n",
        "test_queries = test_data['query'].values\n",
        "test_pids = test_data['pid'].values.astype(np.int64)\n",
        "test_qids = test_data['qid'].values.astype(np.int64)\n",
        "test_labels = test_data['relevancy'].values.astype(np.float64).astype(np.int64)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "370faac1cdd748a2986f51be052d58ba",
            "357221617b344cbca49f6169ce4ce79c",
            "c4f48d1e7eb147c2bd0dda1e26f9518e",
            "990cb7ebce91462db5846bd82f28be41",
            "b4251e16e5de431fae645212cc6697ca",
            "a895929ba5604444b6faa39b9385d408",
            "042198e1b2ed4a1091cd3bf46727ea92",
            "5a83992adb304f76905ecea68d774eb9",
            "f070ac6788734936b68ee599eeb9d1fb",
            "ff6759a4b8334b11b4ce659f59f5d33b",
            "56f5bdc1b5f0453d881dad1e80f72de4",
            "5cd9f0e6a8d9402d81067c50c0f55ef7",
            "8f93e2f54e2d49aa98842ea596ada03f",
            "e7790a0a0b924ee2a2166f3779b04635",
            "1509220e320742f98493c42700b6eafb",
            "7d5a590ddd80427d9275941d173da78a",
            "71f905022fa04a9fb2600951cf9d25d5",
            "5524c6e0ddea4c6bbb8e1aaa8c290c93",
            "84293a5e8aa74abfbd3425b974b43790",
            "a58d0fe5fe814706b69c48f64dec9d9a",
            "037aa7ec3f3547a2a5a58d322f7cc792",
            "88107ed83c4c416dad7fe250b793796c",
            "86707d28c6cb4d759067e8c666215a97",
            "96fd80053c1d4c49a07041bb812b25fa",
            "4f3d7ddf0fa847268fd31f16cf0754a7",
            "8a275414bc9c4e9eb3b0391e221b487e",
            "9a73623a9bd24451ab8f23571a4a3004",
            "51cb41f944d548b9a24601e93d7a341d",
            "6506fbc049cd4c74a6f1bcfba4be4036",
            "bfffa9c87b1e4523bd11ba23d86c4fd2",
            "011333a3073e42a8bde3278ac43d651a",
            "5f6b5b4c4d9e4085b840f03ba57882be",
            "55aa9404de50414f87d0f2064ff43c8e"
          ]
        },
        "id": "mhdBdwumprNK",
        "outputId": "1869ee79-610a-4f2b-957c-10307b06e4ef"
      },
      "outputs": [],
      "source": [
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\"prajjwal1/bert-tiny\")\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = BertModel.from_pretrained(\"prajjwal1/bert-tiny\").to(device)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DQwMUv9PZNzG"
      },
      "outputs": [],
      "source": [
        "class DataSeq(Dataset):\n",
        "    '''\n",
        "        Dataset for generating tokens of sequences.\n",
        "    '''\n",
        "    def __init__(self, queries, passages, labels):\n",
        "        self.queries = queries\n",
        "        self.passages = passages\n",
        "        self.labels = labels\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        query, passage, label = self.queries[index], self.passages[index], self.labels[index]\n",
        "        \n",
        "        ids_query = tokenizer.batch_encode_plus([query], add_special_tokens=False, padding='max_length', max_length=50, truncation=True)\n",
        "        ids_passage = tokenizer.batch_encode_plus([passage], add_special_tokens=False, padding='max_length', max_length=300, truncation=True)\n",
        "\n",
        "        return np.array([ids_query['input_ids'], ids_query['attention_mask']]), np.array([ids_passage['input_ids'], ids_passage['attention_mask']]), label\n",
        "        # return np.array([ids_query['input_ids'], ids_query['attention_mask']]), np.array([ids_passage['input_ids'], ids_passage['attention_mask']])\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "def generate_embedding(queries, passages, labels, section):\n",
        "    '''\n",
        "        This function is used for generating embeddings using pre-processed data sequences.\n",
        "\n",
        "        ***Note that if GPU memory runs out, we should decrease batchsize\n",
        "\n",
        "        Input:\n",
        "            -sequences: list, processed sequences\n",
        "            -labels: list/np.array, corresponding labels for sequences\n",
        "            -section: str, \"train\", \"test\", \"dev\", indicating which section we are loading.\n",
        "\n",
        "        Output:\n",
        "            -embedding-repre: np.array, embedding representation of selected data set.\n",
        "    '''\n",
        "    bs = 128\n",
        "    dataset = DataSeq(queries=queries, passages=passages, labels=labels)\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=bs, shuffle=False, num_workers=2)\n",
        "\n",
        "    progress_bar = tqdm(dataloader)\n",
        "    all_embedding_q = []\n",
        "    all_embedding_p = []\n",
        "    with torch.no_grad():\n",
        "        for i, (ids_q, ids_p, label) in enumerate(progress_bar):\n",
        "            input_ids_q = torch.squeeze(ids_q[:,0,:,:]).to(device)\n",
        "            attention_mask_q = torch.squeeze(ids_q[:,1,:,:]).to(device)   \n",
        "            query_length = torch.count_nonzero(attention_mask_q, 1).unsqueeze(1)                 \n",
        "            embedding_q = model(input_ids=input_ids_q,attention_mask=attention_mask_q)[0]\n",
        "            attention_mask_q = attention_mask_q.unsqueeze(2).repeat(1, 1, 128) \n",
        "            embedding_q = torch.sum(embedding_q * attention_mask_q, 1) / query_length #bs, embedding_size\n",
        "\n",
        "            input_ids_p = torch.squeeze(ids_p[:,0,:,:]).to(device)\n",
        "            attention_mask_p = torch.squeeze(ids_p[:,1,:,:]).to(device)   \n",
        "            passage_length = torch.count_nonzero(attention_mask_p, 1).unsqueeze(1)                                \n",
        "            embedding_p = model(input_ids=input_ids_p,attention_mask=attention_mask_p)[0]\n",
        "            attention_mask_p = attention_mask_p.unsqueeze(2).repeat(1, 1, 128) \n",
        "            embedding_p = torch.sum(embedding_p * attention_mask_p, 1) / passage_length #bs, embedding_size\n",
        "\n",
        "            all_embedding_q.append(embedding_q.cpu().numpy())\n",
        "            all_embedding_p.append(embedding_p.cpu().numpy())\n",
        "\n",
        "    all_embedding_q = np.concatenate(all_embedding_q[:])\n",
        "    all_embedding_p = np.concatenate(all_embedding_p[:])\n",
        "    np.save('embedding_queries_{0:}.npy'.format(section), all_embedding_q)\n",
        "    np.save('embedding_passages_{0:}.npy'.format(section), all_embedding_p)\n",
        "\n",
        "    return all_embedding_q, all_embedding_p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from scipy import sparse\n",
        "\n",
        "def Average_Precision(true_label):\n",
        "    '''\n",
        "        input: true_label, list, this list contains the truth label of retrieved passage with order. \n",
        "        For example, the score for 5 retrieved passages are S = [0.2, 0.3, 0.4, 0.5, 0.6], the truth labels of these passages are T = [1, 1, 0, 0, 0]\n",
        "        Then the input true_label is sorted T, which is sorted by S, as T' = [0, 0, 0, 1, 1]\n",
        "        This means that L' contains both order information as well as truth label information.\n",
        "    '''\n",
        "    # AP for single query\n",
        "    rela_idx = np.where(true_label == 1)[0]\n",
        "    n_rela_passage = len(rela_idx)\n",
        "    denom = rela_idx + 1\n",
        "    numerator = np.arange(1, n_rela_passage+1)\n",
        "    return (numerator/denom/n_rela_passage).sum()\n",
        "\n",
        "def NDCG(true_label):\n",
        "    '''\n",
        "        Input: true_label, same as the input of Average_Precision.\n",
        "    '''\n",
        "    # NDCG for single query\n",
        "    DCG = np.sum((2**true_label - 1) / np.log2(np.arange(1, len(true_label)+1) + 1))\n",
        "    n_rela_passage = int(np.sum(true_label))\n",
        "    opt_rela_score = np.zeros(len(true_label))\n",
        "    opt_rela_score[:n_rela_passage] = 1\n",
        "    optDCG = np.sum((2**opt_rela_score - 1) / np.log2(np.arange(1, len(true_label)+1) + 1))\n",
        "    return DCG/optDCG if optDCG != 0 else 0\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def text_preprocessing(f):\n",
        "    vocab = dict()\n",
        "    lines = [re.sub(u\"([^\\u0061-\\u007a\\u0030-\\u0039\\u0020])\", \"\", line.strip('\\n').lower()) for line in f]\n",
        "    for line in lines:\n",
        "        line = line.split(' ')\n",
        "        line.remove('') if '' in line else line\n",
        "        for word in line:\n",
        "            if word in vocab:\n",
        "                vocab[word] += 1\n",
        "            else:\n",
        "                vocab[word] = 1\n",
        "    del vocab['']\n",
        "    vocab = sorted(vocab.items(), key = lambda item: item[1], reverse=True)\n",
        "    return vocab\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# read passage\n",
        "whole_passage = []\n",
        "with open('part2/validation_data.tsv', 'r') as f: \n",
        "    i = 0\n",
        "    for line in f.readlines():\n",
        "        if i == 0:\n",
        "            pass\n",
        "            i += 1\n",
        "        else:\n",
        "            line = line.strip('\\n').lower().split('\\t')\n",
        "            line[0] = int(line[0]) #qid\n",
        "            line[1] = int(line[1]) #pid\n",
        "            line[2] = re.sub(u\"([^/u0061-\\u007a\\u0030-\\u0039\\u0020])\", \"\", line[2]) #query  \n",
        "            line[3] = re.sub(u\"([^\\u0061-\\u007a\\u0030-\\u0039\\u0020])\", \"\", line[3]) #passage\n",
        "            line[4] = int(float(line[4])) #relavance score\n",
        "            whole_passage.append(line)\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "whole_passage = np.array(whole_passage)\n",
        "\n",
        "pid = whole_passage[:, 1].astype(np.int64)\n",
        "\n",
        "rank_pid, idx_p = np.unique(pid, return_index=True)\n",
        "\n",
        "whole_passage_p = whole_passage[idx_p]\n",
        "\n",
        "uni_passage = whole_passage_p[:, -2]\n",
        "\n",
        "vocab = text_preprocessing(uni_passage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# remove stop words in vocab\n",
        "num_stop_words = 20\n",
        "new_vocab = np.array(np.array(vocab)[num_stop_words: ])[:, 0]\n",
        "new_vocab_dict = {}\n",
        "\n",
        "for i, word in enumerate(new_vocab):\n",
        "    new_vocab_dict[word] = i"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#construct inverted_idx matrix\n",
        "row,col,data = [],[],[]\n",
        "Ld = []\n",
        "for i, line in enumerate(tqdm(uni_passage)):\n",
        "    line = line.split(' ')\n",
        "    line.remove('') if '' in line else line\n",
        "    Ld.append(len(line))\n",
        "\n",
        "    line_vocab = {}\n",
        "        \n",
        "    for word in line:\n",
        "        #column is in order with the frequency of word in vocabulary\n",
        "        if word in new_vocab_dict:\n",
        "            row.append(i)\n",
        "            col.append(new_vocab_dict[word])\n",
        "            data.append(1)\n",
        "\n",
        "inverted_idx = sparse.csr_matrix((data, (row, col)), shape=(len(uni_passage), len(new_vocab_dict)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# #calculate idf_bm25 of new_vocab \n",
        "num_passage = inverted_idx.shape[0]\n",
        "num_vocab = inverted_idx.shape[1]\n",
        "idf_bm25 = np.zeros(num_vocab)\n",
        "block = 5000\n",
        "\n",
        "for i in tqdm(range(0, (num_vocab // block)*block, block)):\n",
        "    temp = inverted_idx[:, i:i+block].toarray()  \n",
        "    count_zero_temp = np.count_nonzero(temp, axis=0) \n",
        "    idf_bm25[i:i+block] = np.log(((num_passage - count_zero_temp) + 0.5)/ (count_zero_temp + 0.5))\n",
        "    \n",
        "\n",
        "temp = inverted_idx[:, i+block:].toarray()\n",
        "idf_bm25[i+block:] =  np.log((num_passage - np.count_nonzero(temp, axis=0) + 0.5)/ (np.count_nonzero(temp, axis=0) + 0.5))\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "idf_bm25 = np.load('idf_bm25.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "qid = whole_passage[:, 0].astype(np.int64)\n",
        "\n",
        "_, query_idx = np.unique(qid, return_index=True)\n",
        "\n",
        "query = whole_passage[:, 2][query_idx] #ranked query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ranked_qid = qid[np.argsort(qid)]\n",
        "last = ranked_qid[0]\n",
        "counter = []\n",
        "cur_counter = []\n",
        "for i, id in enumerate(tqdm(ranked_qid)):\n",
        "    cur = id\n",
        "    if cur != last:\n",
        "        counter.append(cur_counter)\n",
        "        cur_counter = [i]\n",
        "    else:\n",
        "        cur_counter.append(i)\n",
        "    last = cur\n",
        "counter.append(cur_counter)\n",
        "\n",
        "ranked_qid_whole_passage = whole_passage[np.argsort(qid)]\n",
        "\n",
        "qid_top1000 = []\n",
        "qid_top1000_relavance = []\n",
        "\n",
        "pid_top1000 = ranked_qid_whole_passage[:, 1].astype(np.int64) #ranked qid corresponding pid\n",
        "pid_top1000_relavance = ranked_qid_whole_passage[:, -1].astype(np.int64) #ranked qid core\n",
        "\n",
        "for count in tqdm(counter):\n",
        "    qid_top1000.append(pid_top1000[count]) #takeout corresponding 1000 candidate\n",
        "    qid_top1000_relavance.append(pid_top1000_relavance[count]) #take out corresponding 1000 candidate relavance score\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cal_score_bm25(query, idf_bm25, inverted_idx, new_vocab_dict, rank_pid, Ld):\n",
        "    #hyperparameters for BM25\n",
        "    k1 = 1.2\n",
        "    k2 = 100\n",
        "    b = 0.75\n",
        "\n",
        "    #initialize store box\n",
        "    res_score = []\n",
        "    res_pid = []\n",
        "    res_qid = []\n",
        "    res_relavance = []\n",
        "\n",
        "    m_ap = 0\n",
        "    m_ndcg = 0\n",
        "\n",
        "    # deal with one query at a time\n",
        "    for i, line in enumerate(tqdm(query)):\n",
        "        line = line.split(' ')\n",
        "        line.remove('') if '' in line else line\n",
        "\n",
        "        temp = np.zeros(inverted_idx.shape[1])\n",
        "\n",
        "        line_vocab = {}\n",
        "\n",
        "        #construct small dictionary for one single query\n",
        "        for word in line:\n",
        "            if word in line_vocab:\n",
        "                line_vocab[word] += 1\n",
        "            else:\n",
        "                line_vocab[word] = 1\n",
        "\n",
        "        #record the frequency \n",
        "        for word in line:\n",
        "            if word in new_vocab_dict:\n",
        "                temp[new_vocab_dict[word]] = line_vocab[word]\n",
        "\n",
        "        #calculate the tf for one single query\n",
        "        tf_q = temp\n",
        "        if np.sum(temp) == 0:\n",
        "            print('Too many stop words have been deleted, please change the number of it.')\n",
        "\n",
        "        #take out words in query (those elements have non-zero value in tf_q)\n",
        "        nonzero_idx = np.nonzero(tf_q)[0]\n",
        "        tf_q = tf_q[nonzero_idx]\n",
        "\n",
        "        #take out passages-sub-matrix in corresponding position\n",
        "        p_idf = idf_bm25[nonzero_idx].reshape(1, -1)\n",
        "        \n",
        "        p_tf_doc = inverted_idx[:, nonzero_idx].toarray()\n",
        "\n",
        "        #take out corresponding 1000 passages\n",
        "        candidate_idx = np.zeros(qid_top1000[i].shape[0])\n",
        "        for p, candidate in enumerate(qid_top1000[i]):\n",
        "            cur_idx = np.where(rank_pid == candidate)[0]\n",
        "            candidate_idx[p] = cur_idx\n",
        "        candidate_idx = candidate_idx.astype(np.int64)\n",
        "        p_tf_doc = p_tf_doc[candidate_idx]\n",
        "\n",
        "        temp_rank_pid = qid_top1000[i]\n",
        "        temp_rank_relevance = qid_top1000_relavance[i]\n",
        "        \n",
        "        L = (Ld/np.mean(Ld))[candidate_idx].reshape(-1, 1)\n",
        "\n",
        "        \n",
        "        #calculate score, take out corresponding top 100 pid\n",
        "        nonzero_p_idx = np.nonzero(np.sum(p_tf_doc, axis=1))[0]\n",
        "\n",
        "        temp_score = np.zeros(len(qid_top1000[i]))\n",
        "        temp_score[nonzero_p_idx] = np.sum(p_idf * (k1 + 1) * p_tf_doc[nonzero_p_idx] * (k2 + 1) * tf_q / ((k1*((1-b) + b*(L[nonzero_p_idx])) + p_tf_doc[nonzero_p_idx]) * (k2 + tf_q)), axis=1)\n",
        "        temp_res_pid = temp_rank_pid[np.argsort(temp_score)[::-1][:100]]\n",
        "        temp_res_score = temp_score[np.argsort(temp_score)[::-1][:100]]\n",
        "        temp_res_relavance = temp_rank_relevance[np.argsort(temp_score)[::-1][:100]]\n",
        "\n",
        "\n",
        "        ap = Average_Precision(temp_res_relavance)\n",
        "        m_ap += ap\n",
        "\n",
        "        ndcg = NDCG(temp_res_relavance)\n",
        "        m_ndcg += ndcg\n",
        "        \n",
        "        \n",
        "    return m_ap / len(query), m_ndcg / len(query)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "m_ap, m_ndcg = cal_score_bm25(query, idf_bm25, inverted_idx, new_vocab_dict, rank_pid, Ld)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "m_ap, m_ndcg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RD7hVtshHoMy"
      },
      "source": [
        "# Task2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "D6R_ia3S-Ogt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "if not os.path.exists('embedding_passages_train.npy'):\n",
        "    train_all_embedding_q, train_all_embedding_p = generate_embedding(train_queries, train_passages, train_labels, 'train')\n",
        "    test_all_embedding_q, test_all_embedding_p = generate_embedding(test_queries, test_passages, test_labels, 'test')\n",
        "else:\n",
        "    train_all_embedding_p = np.load('embedding_passages_train.npy')\n",
        "    train_all_embedding_q = np.load('embedding_queries_train.npy')\n",
        "\n",
        "    test_all_embedding_p = np.load('embedding_passages_test.npy')\n",
        "    test_all_embedding_q = np.load('embedding_queries_test.npy')\n",
        "\n",
        "#concatenate passage and query to input into logistic regression model\n",
        "xTr = np.concatenate((train_all_embedding_q, train_all_embedding_p), 1)\n",
        "#add bias term\n",
        "xTr = np.concatenate((xTr, np.ones((xTr.shape[0], 1))), 1)\n",
        "yTr = train_labels\n",
        "\n",
        "del train_all_embedding_p, train_all_embedding_q\n",
        "\n",
        "#concatenate passage and query to input into logistic regression model\n",
        "xTe = np.concatenate((test_all_embedding_q, test_all_embedding_p), 1)\n",
        "#add bias term\n",
        "xTe = np.concatenate((xTe, np.ones((xTe.shape[0], 1))), 1)\n",
        "yTe = test_labels\n",
        "\n",
        "del test_all_embedding_p, test_all_embedding_q\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSze8Whissjx",
        "outputId": "a91f4252-9a93-480d-ef1b-9cb3551d0626"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "441"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Ar_fpF3F5vdT"
      },
      "outputs": [],
      "source": [
        "class LogisticRegression:\n",
        "    def __init__(self, xTr, yTr, lr, epochs=200):\n",
        "        np.random.seed(42)\n",
        "        self.xTr = xTr\n",
        "        self.yTr = yTr\n",
        "        self.w = np.random.rand(xTr.shape[1])\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.mini_batch_size = 256\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        return 1 / (1 + np.exp(x@-self.w))\n",
        "    \n",
        "    def criterion(self, pred, target):\n",
        "        # return 1/len(target) * -(target*(np.log(pred)) + (1-target)*np.log(1-pred)).sum()\n",
        "        return  -(target*(np.log(pred)) + (1-target)*np.log(1-pred)).sum()\n",
        "\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        all_loss = []\n",
        "        for epoch in tqdm(range(self.epochs)):\n",
        "            total_loss = 0\n",
        "            #Obtain data\n",
        "            pred = self.forward(self.xTr)\n",
        "            loss = self.criterion(pred, self.yTr)\n",
        "            grad = -self.lr / len(self.xTr) * np.sum((self.yTr - pred) * self.xTr.T, 1)\n",
        "            self.w -= grad\n",
        "            total_loss += loss\n",
        "\n",
        "            print(\"Epoch:\\t\", epoch, \"Averaged loss of the epoch:\", total_loss.item())\n",
        "            all_loss.append(loss)\n",
        "\n",
        "        return all_loss\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "-GiS-sLULWBB"
      },
      "outputs": [],
      "source": [
        "lr1 = LogisticRegression(xTr, yTr, lr=0.5, epochs=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fac9iPsxLrI7"
      },
      "outputs": [],
      "source": [
        "loss_1 = lr1.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "YTNPNeAQdqx4"
      },
      "outputs": [],
      "source": [
        "lr2 = LogisticRegression(xTr, yTr, lr=5e-2, epochs=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OflzI5okduRU"
      },
      "outputs": [],
      "source": [
        "loss_2 = lr2.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "deFjqdjzQNC5"
      },
      "outputs": [],
      "source": [
        "lr3 = LogisticRegression(xTr, yTr, lr=10, epochs=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FrPHiwITQRWA"
      },
      "outputs": [],
      "source": [
        "loss_3 = lr3.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lr4 = LogisticRegression(xTr, yTr, lr=50, epochs=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loss_4 = lr4.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDbMqP6bTf-U"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(np.arange(1, len(loss_1)+1), loss_1, label='lr = 0.5')\n",
        "plt.plot(np.arange(1, len(loss_2)+1), loss_2, label='lr = 0.05')\n",
        "plt.plot(np.arange(1, len(loss_3)+1), loss_3, label='lr = 10')\n",
        "plt.plot(np.arange(1, len(loss_4)+1), loss_4, label='lr = 50')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Epoch Loss')\n",
        "plt.yscale('log')\n",
        "plt.title('Comparsion on training loss of using different learning rate')\n",
        "plt.legend()\n",
        "plt.savefig('Comparsion_lr.png', dpi=300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ETRWiuQ6Us0-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from scipy import sparse\n",
        "\n",
        "def Average_Precision(true_label):\n",
        "    # AP for single query\n",
        "    rela_idx = np.where(true_label == 1)[0]\n",
        "    n_rela_passage = len(rela_idx)\n",
        "    denom = rela_idx + 1\n",
        "    numerator = np.arange(1, n_rela_passage+1)\n",
        "    return (numerator/denom/n_rela_passage).sum()\n",
        "\n",
        "def NDCG(true_label):\n",
        "    # NDCG for single query\n",
        "    DCG = np.sum((2**true_label - 1) / np.log2(np.arange(1, len(true_label)+1) + 1))\n",
        "    n_rela_passage = int(np.sum(true_label))\n",
        "    opt_rela_score = np.zeros(len(true_label))\n",
        "    opt_rela_score[:n_rela_passage] = 1\n",
        "    optDCG = np.sum((2**opt_rela_score - 1) / np.log2(np.arange(1, len(true_label)+1) + 1))\n",
        "    return DCG/optDCG if optDCG != 0 else 0\n",
        "\n",
        "def mean_metric_lr(test_qids, xTe, yTe, test_pids, lr, write=False):\n",
        "    ranked_qid = test_qids[np.argsort(test_qids)]\n",
        "    last = ranked_qid[0]\n",
        "    counter = []\n",
        "    cur_counter = []\n",
        "    for i, id in enumerate(tqdm(ranked_qid)):\n",
        "        cur = id\n",
        "        if cur != last:\n",
        "            counter.append(cur_counter)\n",
        "            cur_counter = [i]\n",
        "        else:\n",
        "            cur_counter.append(i)\n",
        "        last = cur\n",
        "    counter.append(cur_counter)\n",
        "\n",
        "    xTe = xTe[np.argsort(test_qids)]\n",
        "    yTe = yTe[np.argsort(test_qids)]\n",
        "    test_pids = test_pids[np.argsort(test_qids)]\n",
        "    uni_qids = np.unique(test_qids)\n",
        "    \n",
        "    m_ap = 0\n",
        "    m_ndcg = 0\n",
        "\n",
        "    res_qid = []\n",
        "    res_pid = []\n",
        "    res_score = []\n",
        "    res_rank = []\n",
        "    res_A1 = []\n",
        "    res_algoname = []\n",
        "\n",
        "    for i, count in enumerate(tqdm(counter)):\n",
        "        sub_xTe = xTe[count]\n",
        "        pred_yTe = lr.forward(sub_xTe)\n",
        "        sort_idx = np.argsort(pred_yTe)[::-1][:]\n",
        "        sub_yTe = yTe[count]\n",
        "        true_label = sub_yTe[sort_idx]\n",
        "        ap = Average_Precision(true_label)\n",
        "        ndcg = NDCG(true_label)\n",
        "        m_ap += ap\n",
        "        m_ndcg += ndcg\n",
        "\n",
        "        sub_pids = test_pids[count]\n",
        "\n",
        "        res_qid.extend([uni_qids[i] for _ in range(len(sort_idx))])\n",
        "        res_pid.extend(sub_pids[sort_idx])\n",
        "        res_score.extend(pred_yTe[np.argsort(pred_yTe)[::-1][:]])\n",
        "        res_rank.extend(np.arange(1, len(sort_idx)+1))\n",
        "        res_A1.extend(['A1' for _ in range(len(sort_idx))])\n",
        "        res_algoname.extend(['LR' for _ in range(len(sort_idx))])\n",
        "    \n",
        "    if write:\n",
        "        data = {'qid': res_qid, 'A1': res_A1, 'pid': res_pid, 'rank': res_rank, 'score': res_score, 'algoname': res_algoname}\n",
        "        data_df = pd.DataFrame(data)\n",
        "        data_df.to_csv('LR.txt',index=False,header=False, sep=' ')\n",
        "\n",
        "    \n",
        "    return m_ap / len(counter), m_ndcg / len(counter)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PMCj5NPWevt",
        "outputId": "da13b342-02aa-4ceb-9d3a-0ce7d421c027"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1103039/1103039 [00:00<00:00, 1847848.67it/s]\n",
            "100%|██████████| 1148/1148 [00:04<00:00, 279.86it/s]\n"
          ]
        }
      ],
      "source": [
        "m_ap_1, m_ndcg_1 = mean_metric_lr(test_qids, xTe, yTe, test_pids, lr1, write=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "gBuDvAgf-n3C"
      },
      "outputs": [],
      "source": [
        "del xTr, yTr, xTe, yTe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXHqCGk-Me62"
      },
      "source": [
        "# Task3 make sure to restart kernel!!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "agQUCQXpkQyE"
      },
      "outputs": [],
      "source": [
        "!pip install bayesian-optimization\n",
        "from bayes_opt import BayesianOptimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hDMebwZD6Hsn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm \n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "col=['qid','pid','query','passage','relevancy']\n",
        "\n",
        "train_data=pd.read_csv(\"part2/train_data.tsv\", sep='\\t', header=None, names=col)\n",
        "train_data=pd.DataFrame(train_data)\n",
        "train_data = train_data.iloc[1:]\n",
        "\n",
        "train_passages = train_data['passage'].values\n",
        "train_queries = train_data['query'].values\n",
        "train_pids = train_data['pid'].values.astype(np.int64)\n",
        "train_qids = train_data['qid'].values.astype(np.int64)\n",
        "train_labels = train_data['relevancy'].values.astype(np.float64).astype(np.int64)\n",
        "\n",
        "\n",
        "\n",
        "test_data=pd.read_csv(\"part2/validation_data.tsv\", sep='\\t', header=None, names=col)\n",
        "test_data=pd.DataFrame(test_data)\n",
        "test_data = test_data.iloc[1:]\n",
        "\n",
        "test_passages = test_data['passage'].values\n",
        "test_queries = test_data['query'].values\n",
        "test_pids = test_data['pid'].values.astype(np.int64)\n",
        "test_qids = test_data['qid'].values.astype(np.int64)\n",
        "test_labels = test_data['relevancy'].values.astype(np.float64).astype(np.int64)\n",
        "\n",
        "\n",
        "train_all_embedding_p = np.load('embedding_passages_train.npy')\n",
        "train_all_embedding_q = np.load('embedding_queries_train.npy')\n",
        "\n",
        "test_all_embedding_p = np.load('embedding_passages_test.npy')\n",
        "test_all_embedding_q = np.load('embedding_queries_test.npy')\n",
        "\n",
        "#concatenate passage and query to input into logistic regression model\n",
        "xTr = np.concatenate((train_all_embedding_q, train_all_embedding_p), 1)\n",
        "yTr = train_labels\n",
        "\n",
        "#concatenate passage and query to input into logistic regression model\n",
        "xTe = np.concatenate((test_all_embedding_q, test_all_embedding_p), 1)\n",
        "yTe = test_labels\n",
        "\n",
        "del train_all_embedding_p, train_all_embedding_q, test_all_embedding_p, test_all_embedding_q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SEj4rzFc-fF1"
      },
      "outputs": [],
      "source": [
        "#Negative sampling\n",
        "import numpy as np\n",
        "def neg_sampling(qids, xTr, yTr, ratio=5):\n",
        "    ranked_qid = qids[np.argsort(qids)]\n",
        "    yTr = yTr[np.argsort(qids)]\n",
        "    xTr = xTr[np.argsort(qids)]\n",
        "\n",
        "    last = ranked_qid[0]\n",
        "    counter = []\n",
        "    cur_counter = []\n",
        "    for i, id in enumerate(tqdm(ranked_qid)):\n",
        "        cur = id\n",
        "        if cur != last:\n",
        "            counter.append(cur_counter)\n",
        "            cur_counter = [i]\n",
        "        else:\n",
        "            cur_counter.append(i)\n",
        "        last = cur\n",
        "    counter.append(cur_counter)\n",
        "    \n",
        "\n",
        "    sample_xTr = []\n",
        "    sample_yTr = []\n",
        "    sample_qids = []\n",
        "\n",
        "    for i, qid in enumerate(tqdm(np.unique(qids))):\n",
        "        idx1 = np.where(yTr[counter[i]] == 1)\n",
        "        num_pos = idx1[0].shape[0]\n",
        "\n",
        "        #positive sample\n",
        "        sample_xTr.extend(xTr[counter[i]][idx1])\n",
        "        sample_yTr.extend(yTr[counter[i]][idx1])\n",
        "        sample_qids.extend([qid]*(len(xTr[counter[i]][idx1])))\n",
        "\n",
        "        #negative sample\n",
        "        idx0 = np.delete(np.arange(len(counter[i])), idx1)\n",
        "        np.random.shuffle(idx0)\n",
        "        sample_xTr.extend(xTr[counter[i]][idx0[:ratio*num_pos]])\n",
        "        sample_yTr.extend(yTr[counter[i]][idx0[:ratio*num_pos]])\n",
        "        sample_qids.extend([qid]*len(xTr[counter[i]][idx0[:ratio*num_pos]]))\n",
        "\n",
        "    return sample_xTr, sample_yTr, sample_qids\n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtHImK9m_L5U",
        "outputId": "e92ef8df-94b7-47eb-a592-4aa67d3562ce"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4364339/4364339 [00:01<00:00, 2203554.09it/s]\n",
            "100%|██████████| 4590/4590 [00:06<00:00, 694.95it/s]\n"
          ]
        }
      ],
      "source": [
        "sample_xTr, sample_yTr, sample_qids = neg_sampling(train_qids, xTr, yTr, ratio=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zfBEOdue_Xh0"
      },
      "outputs": [],
      "source": [
        "del xTr, yTr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ioIVayYAN9LO"
      },
      "outputs": [],
      "source": [
        "#Negative sampling\n",
        "import numpy as np\n",
        "def Average_Precision(true_label):\n",
        "    # AP for single query\n",
        "    rela_idx = np.where(true_label == 1)[0]\n",
        "    n_rela_passage = len(rela_idx)\n",
        "    denom = rela_idx + 1\n",
        "    numerator = np.arange(1, n_rela_passage+1)\n",
        "    return (numerator/denom/n_rela_passage).sum()\n",
        "\n",
        "def NDCG(true_label):\n",
        "    # NDCG for single query\n",
        "    DCG = np.sum((2**true_label - 1) / np.log2(np.arange(1, len(true_label)+1) + 1))\n",
        "    n_rela_passage = int(np.sum(true_label))\n",
        "    opt_rela_score = np.zeros(len(true_label))\n",
        "    opt_rela_score[:n_rela_passage] = 1\n",
        "    optDCG = np.sum((2**opt_rela_score - 1) / np.log2(np.arange(1, len(true_label)+1) + 1))\n",
        "    return DCG/optDCG.sum() if optDCG != 0 else 0\n",
        "\n",
        "def make_group(qids, yTr, xTr):\n",
        "    xTr = np.array(xTr)\n",
        "    yTr = np.array(yTr)\n",
        "    qids = np.array(qids)\n",
        "\n",
        "    ranked_qid = qids\n",
        "\n",
        "    # ranked_qid = qids[np.argsort(qids)]\n",
        "    # yTr = yTr[np.argsort(qids)]\n",
        "    # xTr = xTr[np.argsort(qids)]\n",
        "\n",
        "\n",
        "    last = ranked_qid[0]\n",
        "    counter = []\n",
        "    cur_counter = []\n",
        "    for i, id in enumerate(tqdm(ranked_qid)):\n",
        "        cur = id\n",
        "        if cur != last:\n",
        "            counter.append(cur_counter)\n",
        "            cur_counter = [i]\n",
        "        else:\n",
        "            cur_counter.append(i)\n",
        "        last = cur\n",
        "    counter.append(cur_counter)\n",
        "\n",
        "    group = [len(counter[i]) for i in range(len(counter))]\n",
        "\n",
        "    return xTr, yTr, group\n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXVyqOqPjuts",
        "outputId": "d4476c9f-6cdd-4cff-e5b1-cfa1646aea12"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28761/28761 [00:00<00:00, 1747814.04it/s]\n"
          ]
        }
      ],
      "source": [
        "xTr, yTr, groups = make_group(sample_qids, sample_yTr, sample_xTr)\n",
        "# xTr, yTr, groups = make_group(train_qids, yTr, xTr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "py2CLmPxUtsV"
      },
      "outputs": [],
      "source": [
        "def xgb_cv(lr, colsample_bytree, max_depth, n_estimators, subsample):\n",
        "    qids = sample_qids\n",
        "    folds = 5\n",
        "    n = len(groups)\n",
        "    c_len = n // folds\n",
        "    group_chunks = [groups[i*c_len:(i+1)*c_len] for i in range(folds-1)]\n",
        "    group_chunks.append(groups[(folds-1)*c_len:])\n",
        "\n",
        "    xTr_chunks = []\n",
        "    yTr_chunks = []\n",
        "    qid_chunks = []\n",
        "    for i in range(folds):\n",
        "        start_idx = 0\n",
        "        chunk_length = sum(group_chunks[i])\n",
        "        xTr_chunks.append(xTr[start_idx:start_idx+chunk_length])\n",
        "        yTr_chunks.append(yTr[start_idx:start_idx+chunk_length])\n",
        "        qid_chunks.append(qids[start_idx:start_idx+chunk_length])\n",
        "        start_idx += chunk_length\n",
        "    \n",
        "    ndcg_list = []\n",
        "    ap_list = []\n",
        "    for i in range(folds):\n",
        "        idx_list = np.arange(folds)\n",
        "        idx_list = np.delete(idx_list, i)\n",
        "\n",
        "        xVal_cv = xTr_chunks[i]\n",
        "        yVal_cv = yTr_chunks[i]\n",
        "        gVal_cv = group_chunks[i]\n",
        "        qids_cv = qid_chunks[i]\n",
        "\n",
        "        xTr_cv = np.concatenate([xTr_chunks[j] for j in idx_list][:])\n",
        "        yTr_cv = np.concatenate([yTr_chunks[j] for j in idx_list][:])\n",
        "        group_cv = np.concatenate([group_chunks[j] for j in idx_list][:])\n",
        "\n",
        "        rank_model = xgb.XGBRanker(  \n",
        "            tree_method='gpu_hist',\n",
        "            booster='gbtree',\n",
        "            objective='rank:pairwise',\n",
        "            random_state=42, \n",
        "            learning_rate=lr,\n",
        "            colsample_bytree=colsample_bytree, \n",
        "            eta=0.05, \n",
        "            max_depth=int(max_depth), \n",
        "            n_estimators=int(n_estimators), \n",
        "            subsample=subsample,\n",
        "            )\n",
        "        rank_model.fit(xTr_cv, yTr_cv, group_cv, verbose=True)\n",
        "\n",
        "\n",
        "        ranked_qid = qids_cv\n",
        "\n",
        "        last = ranked_qid[0]\n",
        "        counter = []\n",
        "        cur_counter = []\n",
        "        for i, id in enumerate(tqdm(ranked_qid)):\n",
        "            cur = id\n",
        "            if cur != last:\n",
        "                counter.append(cur_counter)\n",
        "                cur_counter = [i]\n",
        "            else:\n",
        "                cur_counter.append(i)\n",
        "            last = cur\n",
        "        counter.append(cur_counter)\n",
        "\n",
        "        m_ap = 0\n",
        "        m_ndcg = 0\n",
        "\n",
        "\n",
        "        for count in tqdm(counter):\n",
        "            sub_xTe = xVal_cv[count]\n",
        "            pred_yTe = rank_model.predict(sub_xTe)\n",
        "            sort_idx = np.argsort(pred_yTe)[::-1]\n",
        "            sub_yTe = yVal_cv[count]\n",
        "            true_label = sub_yTe[sort_idx]\n",
        "            ap = Average_Precision(true_label)\n",
        "            ndcg = NDCG(true_label)\n",
        "            m_ap += ap\n",
        "            m_ndcg += ndcg\n",
        "\n",
        "\n",
        "        \n",
        "        ndcg_list.append(m_ndcg/len(counter))\n",
        "        ap_list.append(m_ap/len(counter))\n",
        "\n",
        "    return np.mean(ap_list)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "ykYi2QzRg87d"
      },
      "outputs": [],
      "source": [
        "xgb_bo = BayesianOptimization(\n",
        "    xgb_cv,\n",
        "    {'lr': (0.001, 0.1),\n",
        "    'colsample_bytree': (0.5, 0.9),\n",
        "    'max_depth': (4, 10),\n",
        "    'n_estimators': (300, 800),\n",
        "     'subsample': (0.5, 0.9)})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mM5NqCfhcTY",
        "outputId": "233ba509-4b50-4e7d-f3dc-5c57854f3460"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "|   iter    |  target   | colsam... |    lr     | max_depth | n_esti... | subsample |\n",
            "-------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5784/5784 [00:00<00:00, 1462053.54it/s]\n",
            "100%|██████████| 918/918 [00:24<00:00, 37.64it/s]\n",
            "100%|██████████| 5860/5860 [00:00<00:00, 1521519.22it/s]\n",
            "100%|██████████| 930/930 [00:24<00:00, 37.68it/s]\n",
            "100%|██████████| 5691/5691 [00:00<00:00, 1532472.01it/s]\n",
            "100%|██████████| 903/903 [00:24<00:00, 37.61it/s]\n",
            "100%|██████████| 5696/5696 [00:00<00:00, 1757319.28it/s]\n",
            "100%|██████████| 904/904 [00:24<00:00, 37.60it/s]\n",
            "100%|██████████| 5730/5730 [00:00<00:00, 1672118.69it/s]\n",
            "100%|██████████| 909/909 [00:24<00:00, 37.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.9353  \u001b[0m | \u001b[0m 0.5045  \u001b[0m | \u001b[0m 0.001323\u001b[0m | \u001b[0m 5.907   \u001b[0m | \u001b[0m 740.4   \u001b[0m | \u001b[0m 0.8127  \u001b[0m |\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5784/5784 [00:00<00:00, 1538159.67it/s]\n",
            "100%|██████████| 918/918 [00:10<00:00, 89.64it/s]\n",
            "100%|██████████| 5860/5860 [00:00<00:00, 1580415.47it/s]\n",
            "100%|██████████| 930/930 [00:10<00:00, 89.42it/s]\n",
            "100%|██████████| 5691/5691 [00:00<00:00, 1686791.33it/s]\n",
            "100%|██████████| 903/903 [00:10<00:00, 89.30it/s]\n",
            "100%|██████████| 5696/5696 [00:00<00:00, 1587531.10it/s]\n",
            "100%|██████████| 904/904 [00:10<00:00, 89.52it/s]\n",
            "100%|██████████| 5730/5730 [00:00<00:00, 1747499.59it/s]\n",
            "100%|██████████| 909/909 [00:10<00:00, 89.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.9974  \u001b[0m | \u001b[95m 0.7481  \u001b[0m | \u001b[95m 0.05633 \u001b[0m | \u001b[95m 5.295   \u001b[0m | \u001b[95m 310.0   \u001b[0m | \u001b[95m 0.5534  \u001b[0m |\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5784/5784 [00:00<00:00, 1481246.45it/s]\n",
            "100%|██████████| 918/918 [00:12<00:00, 74.08it/s]\n",
            "100%|██████████| 5860/5860 [00:00<00:00, 1744278.01it/s]\n",
            "100%|██████████| 930/930 [00:12<00:00, 74.18it/s]\n",
            "100%|██████████| 5691/5691 [00:00<00:00, 1700490.42it/s]\n",
            "100%|██████████| 903/903 [00:12<00:00, 74.22it/s]\n",
            "100%|██████████| 5696/5696 [00:00<00:00, 1764848.61it/s]\n",
            "100%|██████████| 904/904 [00:12<00:00, 73.91it/s]\n",
            "100%|██████████| 5730/5730 [00:00<00:00, 1534991.50it/s]\n",
            "100%|██████████| 909/909 [00:12<00:00, 73.90it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.9985  \u001b[0m | \u001b[95m 0.6235  \u001b[0m | \u001b[95m 0.05591 \u001b[0m | \u001b[95m 6.155   \u001b[0m | \u001b[95m 310.9   \u001b[0m | \u001b[95m 0.8496  \u001b[0m |\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5784/5784 [00:00<00:00, 1424369.09it/s]\n",
            "100%|██████████| 918/918 [00:25<00:00, 35.71it/s]\n",
            "100%|██████████| 5860/5860 [00:00<00:00, 1843166.21it/s]\n",
            "100%|██████████| 930/930 [00:26<00:00, 35.72it/s]\n",
            "100%|██████████| 5691/5691 [00:00<00:00, 1612169.67it/s]\n",
            "100%|██████████| 903/903 [00:25<00:00, 35.60it/s]\n",
            "100%|██████████| 5696/5696 [00:00<00:00, 1666486.86it/s]\n",
            "100%|██████████| 904/904 [00:25<00:00, 35.59it/s]\n",
            "100%|██████████| 5730/5730 [00:00<00:00, 1660565.32it/s]\n",
            "100%|██████████| 909/909 [00:25<00:00, 35.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| \u001b[95m 4       \u001b[0m | \u001b[95m 0.9986  \u001b[0m | \u001b[95m 0.6878  \u001b[0m | \u001b[95m 0.03383 \u001b[0m | \u001b[95m 10.0    \u001b[0m | \u001b[95m 377.6   \u001b[0m | \u001b[95m 0.9     \u001b[0m |\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5784/5784 [00:00<00:00, 1340101.33it/s]\n",
            "100%|██████████| 918/918 [00:27<00:00, 33.10it/s]\n",
            "100%|██████████| 5860/5860 [00:00<00:00, 1675777.01it/s]\n",
            "100%|██████████| 930/930 [00:28<00:00, 32.86it/s]\n",
            "100%|██████████| 5691/5691 [00:00<00:00, 1739526.60it/s]\n",
            "100%|██████████| 903/903 [00:27<00:00, 32.85it/s]\n",
            "100%|██████████| 5696/5696 [00:00<00:00, 1750623.26it/s]\n",
            "100%|██████████| 904/904 [00:27<00:00, 32.95it/s]\n",
            "100%|██████████| 5730/5730 [00:00<00:00, 1675032.19it/s]\n",
            "100%|██████████| 909/909 [00:27<00:00, 32.89it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.9985  \u001b[0m | \u001b[0m 0.5301  \u001b[0m | \u001b[0m 0.08357 \u001b[0m | \u001b[0m 9.936   \u001b[0m | \u001b[0m 492.1   \u001b[0m | \u001b[0m 0.7412  \u001b[0m |\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5784/5784 [00:00<00:00, 1540699.50it/s]\n",
            "100%|██████████| 918/918 [00:14<00:00, 62.90it/s]\n",
            "100%|██████████| 5860/5860 [00:00<00:00, 1485831.30it/s]\n",
            "100%|██████████| 930/930 [00:14<00:00, 62.88it/s]\n",
            "100%|██████████| 5691/5691 [00:00<00:00, 1734848.76it/s]\n",
            "100%|██████████| 903/903 [00:14<00:00, 62.89it/s]\n",
            "100%|██████████| 5696/5696 [00:00<00:00, 1745507.09it/s]\n",
            "100%|██████████| 904/904 [00:14<00:00, 62.76it/s]\n",
            "100%|██████████| 5730/5730 [00:00<00:00, 1622450.68it/s]\n",
            "100%|██████████| 909/909 [00:14<00:00, 62.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.9982  \u001b[0m | \u001b[0m 0.7969  \u001b[0m | \u001b[0m 0.09722 \u001b[0m | \u001b[0m 4.047   \u001b[0m | \u001b[0m 571.2   \u001b[0m | \u001b[0m 0.679   \u001b[0m |\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5784/5784 [00:00<00:00, 1381540.68it/s]\n",
            "100%|██████████| 918/918 [00:11<00:00, 79.12it/s]\n",
            "100%|██████████| 5860/5860 [00:00<00:00, 1697653.09it/s]\n",
            "100%|██████████| 930/930 [00:11<00:00, 78.93it/s]\n",
            "100%|██████████| 5691/5691 [00:00<00:00, 1625674.87it/s]\n",
            "100%|██████████| 903/903 [00:11<00:00, 79.11it/s]\n",
            "100%|██████████| 5696/5696 [00:00<00:00, 1640961.30it/s]\n",
            "100%|██████████| 904/904 [00:11<00:00, 79.10it/s]\n",
            "100%|██████████| 5730/5730 [00:00<00:00, 1665859.98it/s]\n",
            "100%|██████████| 909/909 [00:11<00:00, 79.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.7851  \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.001   \u001b[0m | \u001b[0m 4.0     \u001b[0m | \u001b[0m 436.7   \u001b[0m | \u001b[0m 0.9     \u001b[0m |\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5784/5784 [00:00<00:00, 1311981.74it/s]\n",
            "100%|██████████| 918/918 [00:28<00:00, 32.11it/s]\n",
            "100%|██████████| 5860/5860 [00:00<00:00, 1716983.68it/s]\n",
            "100%|██████████| 930/930 [00:29<00:00, 31.92it/s]\n",
            "100%|██████████| 5691/5691 [00:00<00:00, 1332688.52it/s]\n",
            "100%|██████████| 903/903 [00:28<00:00, 32.10it/s]\n",
            "100%|██████████| 5696/5696 [00:00<00:00, 1741435.64it/s]\n",
            "100%|██████████| 904/904 [00:28<00:00, 31.71it/s]\n",
            "100%|██████████| 5730/5730 [00:00<00:00, 1231469.66it/s]\n",
            "100%|██████████| 909/909 [00:28<00:00, 31.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.9986  \u001b[0m | \u001b[0m 0.7535  \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 530.1   \u001b[0m | \u001b[0m 0.5     \u001b[0m |\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5784/5784 [00:00<00:00, 1390249.53it/s]\n",
            "100%|██████████| 918/918 [00:36<00:00, 25.11it/s]\n",
            "100%|██████████| 5860/5860 [00:00<00:00, 1822258.41it/s]\n",
            "100%|██████████| 930/930 [00:37<00:00, 25.12it/s]\n",
            "100%|██████████| 5691/5691 [00:00<00:00, 1776818.82it/s]\n",
            "100%|██████████| 903/903 [00:35<00:00, 25.10it/s]\n",
            "100%|██████████| 5696/5696 [00:00<00:00, 1374849.26it/s]\n",
            "100%|██████████| 904/904 [00:35<00:00, 25.14it/s]\n",
            "100%|██████████| 5730/5730 [00:00<00:00, 1802141.72it/s]\n",
            "100%|██████████| 909/909 [00:36<00:00, 25.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.9985  \u001b[0m | \u001b[0m 0.514   \u001b[0m | \u001b[0m 0.04823 \u001b[0m | \u001b[0m 9.832   \u001b[0m | \u001b[0m 624.0   \u001b[0m | \u001b[0m 0.5077  \u001b[0m |\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5784/5784 [00:00<00:00, 1006967.22it/s]\n",
            "100%|██████████| 918/918 [00:16<00:00, 54.01it/s]\n",
            "100%|██████████| 5860/5860 [00:00<00:00, 1397703.81it/s]\n",
            "100%|██████████| 930/930 [00:17<00:00, 53.85it/s]\n",
            "100%|██████████| 5691/5691 [00:00<00:00, 1886492.06it/s]\n",
            "100%|██████████| 903/903 [00:16<00:00, 53.76it/s]\n",
            "100%|██████████| 5696/5696 [00:00<00:00, 1771391.38it/s]\n",
            "100%|██████████| 904/904 [00:16<00:00, 53.90it/s]\n",
            "100%|██████████| 5730/5730 [00:00<00:00, 1706187.84it/s]\n",
            "100%|██████████| 909/909 [00:16<00:00, 53.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.9984  \u001b[0m | \u001b[0m 0.7659  \u001b[0m | \u001b[0m 0.07296 \u001b[0m | \u001b[0m 4.062   \u001b[0m | \u001b[0m 670.2   \u001b[0m | \u001b[0m 0.7748  \u001b[0m |\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5784/5784 [00:00<00:00, 1500300.21it/s]\n",
            "100%|██████████| 918/918 [00:09<00:00, 101.25it/s]\n",
            "100%|██████████| 5860/5860 [00:00<00:00, 1711841.58it/s]\n",
            "100%|██████████| 930/930 [00:09<00:00, 101.26it/s]\n",
            "100%|██████████| 5691/5691 [00:00<00:00, 1460461.58it/s]\n",
            "100%|██████████| 903/903 [00:08<00:00, 100.94it/s]\n",
            "100%|██████████| 5696/5696 [00:00<00:00, 1694019.40it/s]\n",
            "100%|██████████| 904/904 [00:08<00:00, 101.19it/s]\n",
            "100%|██████████| 5730/5730 [00:00<00:00, 1759654.56it/s]\n",
            "100%|██████████| 909/909 [00:08<00:00, 101.01it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.9973  \u001b[0m | \u001b[0m 0.9     \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 4.0     \u001b[0m | \u001b[0m 347.0   \u001b[0m | \u001b[0m 0.5     \u001b[0m |\n",
            "=====================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import xgboost as xgb\n",
        "xgb_bo.maximize(init_points=0,n_iter=10,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLQ5hmKjxKWM",
        "outputId": "30413187-374b-4f87-cd7f-c93b84573913"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'params': {'colsample_bytree': 0.6878421461159999,\n",
              "  'lr': 0.033832164874053106,\n",
              "  'max_depth': 10.0,\n",
              "  'n_estimators': 377.64193463828013,\n",
              "  'subsample': 0.9},\n",
              " 'target': 0.9986408090117767}"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "xgb_bo.max"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nLo9N-B5pdSq"
      },
      "outputs": [],
      "source": [
        "#Negative sampling\n",
        "import numpy as np\n",
        "def Average_Precision(true_label):\n",
        "    # AP for single query\n",
        "    rela_idx = np.where(true_label == 1)[0]\n",
        "    n_rela_passage = len(rela_idx)\n",
        "    denom = rela_idx + 1\n",
        "    numerator = np.arange(1, n_rela_passage+1)\n",
        "    return (numerator/denom/n_rela_passage).sum()\n",
        "\n",
        "def NDCG(true_label):\n",
        "    # NDCG for single query\n",
        "    DCG = np.sum((2**true_label - 1) / np.log2(np.arange(1, len(true_label)+1) + 1))\n",
        "    n_rela_passage = int(np.sum(true_label))\n",
        "    opt_rela_score = np.zeros(len(true_label))\n",
        "    opt_rela_score[:n_rela_passage] = 1\n",
        "    optDCG = np.sum((2**opt_rela_score - 1) / np.log2(np.arange(1, len(true_label)+1) + 1))\n",
        "    return DCG/optDCG.sum() if optDCG != 0 else 0\n",
        "\n",
        "def make_group(qids, yTr, xTr):\n",
        "    xTr = np.array(xTr)\n",
        "    yTr = np.array(yTr)\n",
        "    qids = np.array(qids)\n",
        "\n",
        "    # ranked_qid = qids\n",
        "\n",
        "    ranked_qid = qids[np.argsort(qids)]\n",
        "    yTr = yTr[np.argsort(qids)]\n",
        "    xTr = xTr[np.argsort(qids)]\n",
        "\n",
        "\n",
        "    last = ranked_qid[0]\n",
        "    counter = []\n",
        "    cur_counter = []\n",
        "    for i, id in enumerate(tqdm(ranked_qid)):\n",
        "        cur = id\n",
        "        if cur != last:\n",
        "            counter.append(cur_counter)\n",
        "            cur_counter = [i]\n",
        "        else:\n",
        "            cur_counter.append(i)\n",
        "        last = cur\n",
        "    counter.append(cur_counter)\n",
        "\n",
        "    group = [len(counter[i]) for i in range(len(counter))]\n",
        "\n",
        "    return xTr, yTr, group\n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30x5s308piHQ",
        "outputId": "9c0b5842-d6b8-401f-df95-3306997b9582"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4364339/4364339 [00:01<00:00, 2214036.59it/s]\n"
          ]
        }
      ],
      "source": [
        "xTr, yTr, groups = make_group(train_qids, yTr, xTr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZ9Inqs8xi7a",
        "outputId": "1f31aa7d-54e2-4cb2-d0aa-a09018308454"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1103039/1103039 [00:00<00:00, 2156566.94it/s]\n",
            "100%|██████████| 1148/1148 [00:59<00:00, 19.16it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.03581239563558507, 0.16844698828579643)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import xgboost as xgb\n",
        "rank_model = xgb.XGBRanker(  \n",
        "    tree_method='gpu_hist',\n",
        "    booster='gbtree',\n",
        "    objective='rank:pairwise',\n",
        "    random_state=42, \n",
        "    learning_rate=0.033832164874053106,\n",
        "    colsample_bytree=0.6878421461159999, \n",
        "    eta=0.05, \n",
        "    max_depth=10, \n",
        "    n_estimators=378, \n",
        "    subsample=0.9,\n",
        "    )\n",
        "\n",
        "rank_model.fit(xTr, yTr, group=groups, verbose=True)\n",
        "\n",
        "def mean_metric_xgb(rank_model, test_qids, test_pids, xTe, yTe, write=True):\n",
        "    ranked_qid = test_qids[np.argsort(test_qids)]\n",
        "    last = ranked_qid[0]\n",
        "    counter = []\n",
        "    cur_counter = []\n",
        "    for i, id in enumerate(tqdm(ranked_qid)):\n",
        "        cur = id\n",
        "        if cur != last:\n",
        "            counter.append(cur_counter)\n",
        "            cur_counter = [i]\n",
        "        else:\n",
        "            cur_counter.append(i)\n",
        "        last = cur\n",
        "    counter.append(cur_counter)\n",
        "\n",
        "    xTe = xTe[np.argsort(test_qids)]\n",
        "    yTe = yTe[np.argsort(test_qids)]\n",
        "    test_pids = test_pids[np.argsort(test_qids)]\n",
        "    uni_qids = np.unique(test_qids)\n",
        "\n",
        "\n",
        "    m_ap = 0\n",
        "    m_ndcg = 0\n",
        "\n",
        "    res_qid = []\n",
        "    res_pid = []\n",
        "    res_score = []\n",
        "    res_rank = []\n",
        "    res_A1 = []\n",
        "    res_algoname = []\n",
        "\n",
        "    for i, count in enumerate(tqdm(counter)):\n",
        "        sub_xTe = xTe[count]\n",
        "        pred_yTe = rank_model.predict(sub_xTe)\n",
        "        sort_idx = np.argsort(pred_yTe)[::-1][:]\n",
        "        sub_yTe = yTe[count]\n",
        "        true_label = sub_yTe[sort_idx]\n",
        "        ap = Average_Precision(true_label)\n",
        "        ndcg = NDCG(true_label)\n",
        "        m_ap += ap\n",
        "        m_ndcg += ndcg\n",
        "\n",
        "        sub_pids = test_pids[count]\n",
        "\n",
        "        res_qid.extend([uni_qids[i] for _ in range(len(sort_idx))])\n",
        "        res_pid.extend(sub_pids[sort_idx])\n",
        "        res_score.extend(pred_yTe[np.argsort(pred_yTe)[::-1][:]])\n",
        "        res_rank.extend(np.arange(1, len(sort_idx)+1))\n",
        "        res_A1.extend(['A1' for _ in range(len(sort_idx))])\n",
        "        res_algoname.extend(['LM' for _ in range(len(sort_idx))])\n",
        "\n",
        "    if write:\n",
        "        data = {'qid': res_qid, 'A1': res_A1, 'pid': res_pid, 'rank': res_rank, 'score': res_score, 'algoname': res_algoname}\n",
        "        data_df = pd.DataFrame(data)\n",
        "        data_df.to_csv('/content/gdrive/MyDrive/LM.txt',index=False,header=False, sep=' ')\n",
        "    \n",
        "    return m_ap / len(counter), m_ndcg / len(counter)\n",
        "    \n",
        "mean_metric_xgb(rank_model, test_qids, test_pids, xTe, yTe, write='False')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2aKwg6aXYOQ"
      },
      "source": [
        "# Task 4 Make sure to restart kernel!!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oCWlaDJqJmA3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm \n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "col=['qid','pid','query','passage','relevancy']\n",
        "\n",
        "train_data=pd.read_csv(\"part2/train_data.tsv\", sep='\\t', header=None, names=col)\n",
        "train_data=pd.DataFrame(train_data)\n",
        "train_data = train_data.iloc[1:]\n",
        "\n",
        "train_passages = train_data['passage'].values\n",
        "train_queries = train_data['query'].values\n",
        "train_pids = train_data['pid'].values.astype(np.int64)\n",
        "train_qids = train_data['qid'].values.astype(np.int64)\n",
        "train_labels = train_data['relevancy'].values.astype(np.float64).astype(np.int64)\n",
        "\n",
        "\n",
        "\n",
        "test_data=pd.read_csv(\"part2/validation_data.tsv\", sep='\\t', header=None, names=col)\n",
        "test_data=pd.DataFrame(test_data)\n",
        "test_data = test_data.iloc[1:]\n",
        "\n",
        "test_passages = test_data['passage'].values\n",
        "test_queries = test_data['query'].values\n",
        "test_pids = test_data['pid'].values.astype(np.int64)\n",
        "test_qids = test_data['qid'].values.astype(np.int64)\n",
        "test_labels = test_data['relevancy'].values.astype(np.float64).astype(np.int64)\n",
        "\n",
        "class DataSeq(Dataset):\n",
        "    '''\n",
        "        Dataset for generating tokens of sequences.\n",
        "    '''\n",
        "    def __init__(self, queries, passages, labels):\n",
        "        self.queries = queries\n",
        "        self.passages = passages\n",
        "        self.labels = labels\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        query, passage, label = self.queries[index], self.passages[index], self.labels[index]\n",
        "        \n",
        "        # ids_query = tokenizer.batch_encode_plus([query], add_special_tokens=False, padding='max_length', max_length=50, truncation=True, return_tensors='pt').to(device)\n",
        "        # ids_passage = tokenizer.batch_encode_plus([passage], add_special_tokens=False, padding='max_length', max_length=300, truncation=True, return_tensors='pt').to(device)\n",
        "        # return model(**ids_query), model(**ids_passage), ids_query.attention_mask, ids_passage.attention_mask, label\n",
        "\n",
        "        ids_query = tokenizer.batch_encode_plus([query], add_special_tokens=False, padding='max_length', max_length=50, truncation=True)\n",
        "        ids_passage = tokenizer.batch_encode_plus([passage], add_special_tokens=False, padding='max_length', max_length=300, truncation=True)\n",
        "        return np.array([ids_query['input_ids']]), np.array([ids_passage['input_ids']]), np.array([ids_query['attention_mask']]), np.array([ids_passage['attention_mask']]), label\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "nd6e9eM4wjzF"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--hidden_size', type=int, default=128,\n",
        "                    choices=[64, 128, 256])\n",
        "parser.add_argument('--vocab_size', type=int, default=len(tokenizer))\n",
        "parser.add_argument('--device', default=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
        "parser.add_argument('--lr', type=float, default=1e-4)\n",
        "parser.add_argument('--batch_size', type=int, default=128,\n",
        "                    choices=[64, 128, 256])\n",
        "parser.add_argument('--epochs', type=int, default=100)\n",
        "parser.add_argument('--feed_forward', type=float, default=64)\n",
        "parser.add_argument('--dropout', type=float, default=0.1) \n",
        "parser.add_argument('--num_head', type=int, default=4)\n",
        "parser.add_argument('--num_transformer_layer', default=2)\n",
        "args = parser.parse_args(args=[])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-EEOT5ApX5G8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import math\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, args, max_len=350):\n",
        "        super().__init__()\n",
        "        self.args = args\n",
        "        self.embeddings = nn.Embedding(max_len, self.args.hidden_size)\n",
        "        self.register_buffer('position_ids', torch.arange(max_len))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        return (l b d)\n",
        "        \"\"\"\n",
        "        position_ids = self.position_ids[:x.size(0)] #1, seq_len\n",
        "        position_ids = position_ids.repeat(x.shape[1], 1) #bs, seq_len\n",
        "        position_ids = self.embeddings(position_ids) #bs, seq_len, embedding_size\n",
        "        position_ids = position_ids.transpose(0, 1)\n",
        "\n",
        "        return x + position_ids\n",
        "\n",
        "\n",
        "class BinaryClassifier(torch.nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(BinaryClassifier, self).__init__()\n",
        "        self.args = args\n",
        "        ##positional encoder\n",
        "        self.pos_encoder = PositionalEncoding(self.args) \n",
        "        ##Multi-atten\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=self.args.hidden_size, nhead=self.args.num_head, dim_feedforward=self.args.feed_forward, dropout=self.args.dropout)\n",
        "        self.multi_atten = nn.TransformerEncoder(encoder_layer, num_layers=self.args.num_transformer_layer)\n",
        "        \n",
        "        self.fc = nn.Linear(self.args.hidden_size, self.args.hidden_size) \n",
        "        self.dropout = nn.Dropout(self.args.dropout)\n",
        "\n",
        "        self.down1 = nn.Linear(self.args.hidden_size, self.args.hidden_size//4)\n",
        "        self.down2 = nn.Linear(self.args.hidden_size//4, self.args.hidden_size//4)\n",
        "        self.down3 = nn.Linear(self.args.hidden_size//4, self.args.hidden_size//8)\n",
        "        self.out = nn.Linear(self.args.hidden_size//8, 1)\n",
        "\n",
        "        # self.out = nn.Linear(self.args.hidden_size, 1)\n",
        "        \n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "\n",
        "\n",
        "    def forward(self, query, passage, mask_q, mask_p):\n",
        "        #query: bs, seq_len_q, embedding_size\n",
        "        #passage: bs, seq_len_p, embedding_size\n",
        "\n",
        "\n",
        "        x = torch.cat((query, passage), 1) #bs, seq_len_all\n",
        "        mask = torch.cat((mask_q, mask_p), 1) #bs, seq_len_all\n",
        "\n",
        "        #embedding layer\n",
        "        x = x * math.sqrt(self.args.hidden_size) #bs, seq_len_all, embedding_size\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        #multi-atten\n",
        "        x = torch.transpose(x, 0, 1) # seq_len, bs, embedding_size\n",
        "        x = self.pos_encoder(x)\n",
        "        x = self.multi_atten(x, src_key_padding_mask=(mask==0)) # seq_len, bs, embedding_size\n",
        "        x = torch.transpose(x, 0, 1) #bs, seq_len, embedding_size\n",
        "        \n",
        "        #pooling\n",
        "        x = torch.mean(x, 1)\n",
        "        x = self.relu(self.fc(x))\n",
        "\n",
        "        #down\n",
        "        x = self.relu(self.down1(x))\n",
        "        x = self.relu(self.down2(x))\n",
        "        x = self.relu(self.down3(x))\n",
        "        x = self.out(x)\n",
        "\n",
        "        # x = self.out(x)\n",
        "\n",
        "        x = nn.Sigmoid()(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "TaOYFpNs6zUO",
        "outputId": "1ae41bc2-ba78-4640-a10d-6d52175afd30"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n    Parallel encoding for query and passage, respectively, with m_ndcg = 0.14 and m_ap = 0.02.\\n    Model is not performing well because query and passage are not computing attention with each other, thus model is hard to obtain interactive info\\n    between passage and query, thus harder to predict if they are relevant of not.\\n'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "    Parallel encoding for query and passage, respectively, with m_ndcg = 0.14 and m_ap = 0.02.\n",
        "    Model is not performing well because query and passage are not computing attention with each other, thus model is hard to obtain interactive info\n",
        "    between passage and query, thus harder to predict if they are relevant of not.\n",
        "'''\n",
        "# import torch\n",
        "# from torch import nn, optim\n",
        "# import torch.nn.functional as F\n",
        "# import random\n",
        "# import math\n",
        "\n",
        "\n",
        "# class PositionalEncoding(nn.Module):\n",
        "#     def __init__(self, args, max_len=350):\n",
        "#         super().__init__()\n",
        "#         self.args = args\n",
        "#         self.embeddings = nn.Embedding(max_len, self.args.hidden_size)\n",
        "#         self.register_buffer('position_ids', torch.arange(max_len))\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         \"\"\"\n",
        "#         return (l b d)\n",
        "#         \"\"\"\n",
        "#         position_ids = self.position_ids[:x.size(0)] #1, seq_len\n",
        "#         position_ids = position_ids.repeat(x.shape[1], 1) #bs, seq_len\n",
        "#         position_ids = self.embeddings(position_ids) #bs, seq_len, embedding_size\n",
        "#         position_ids = position_ids.transpose(0, 1)\n",
        "\n",
        "#         return x + position_ids\n",
        "\n",
        "\n",
        "# class BinaryClassifier(torch.nn.Module):\n",
        "#     def __init__(self, args):\n",
        "#         super(BinaryClassifier, self).__init__()\n",
        "#         self.args = args\n",
        "#         ##positional encoder\n",
        "#         self.pos_encoder_q = PositionalEncoding(self.args, max_len=50)\n",
        "#         self.pos_encoder_p = PositionalEncoding(self.args, max_len=300)\n",
        "#         ##Multi-atten\n",
        "#         encoder_layer_q = nn.TransformerEncoderLayer(d_model=self.args.hidden_size, nhead=self.args.num_head, dim_feedforward=self.args.feed_forward, dropout=self.args.dropout)\n",
        "#         self.multi_atten_q = nn.TransformerEncoder(encoder_layer_q, num_layers=self.args.num_transformer_layer)\n",
        "\n",
        "#         encoder_layer_p = nn.TransformerEncoderLayer(d_model=self.args.hidden_size, nhead=self.args.num_head, dim_feedforward=self.args.feed_forward, dropout=self.args.dropout)\n",
        "#         self.multi_atten_p = nn.TransformerEncoder(encoder_layer_p, num_layers=self.args.num_transformer_layer)\n",
        "\n",
        "        \n",
        "#         self.fc_p = nn.Linear(self.args.hidden_size, self.args.hidden_size) \n",
        "#         self.fc_q = nn.Linear(self.args.hidden_size, self.args.hidden_size) \n",
        "\n",
        "#         self.dropout = nn.Dropout(self.args.dropout)\n",
        "\n",
        "#         self.down1 = nn.Linear(self.args.hidden_size, self.args.hidden_size//4)\n",
        "#         self.down2 = nn.Linear(self.args.hidden_size//4, self.args.hidden_size//4)\n",
        "#         self.down3 = nn.Linear(self.args.hidden_size//4, self.args.hidden_size//8)\n",
        "#         self.out = nn.Linear(self.args.hidden_size//8, 1)\n",
        "\n",
        "#         # self.out = nn.Linear(self.args.hidden_size, 1)\n",
        "        \n",
        "#         self.relu = nn.ReLU()\n",
        "        \n",
        "\n",
        "\n",
        "#     def forward(self, query, passage, mask_q, mask_p):\n",
        "#         #query: bs, seq_len_q, embedding_size\n",
        "#         #passage: bs, seq_len_p, embedding_size\n",
        "\n",
        "\n",
        "#         #embedding layer\n",
        "#         x_q = query * math.sqrt(self.args.hidden_size) #bs, seq_len_all, embedding_size\n",
        "#         x_q = self.dropout(x_q)\n",
        "\n",
        "#         #multi-atten\n",
        "#         x_q = torch.transpose(x_q, 0, 1) # seq_len, bs, embedding_size\n",
        "#         x_q = self.pos_encoder_q(x_q)\n",
        "#         x_q = self.multi_atten_q(x_q, src_key_padding_mask=(mask_q==0)) # seq_len, bs, embedding_size\n",
        "#         x_q = torch.transpose(x_q, 0, 1) #bs, seq_len, embedding_size\n",
        "        \n",
        "#         #pooling\n",
        "#         x_q = torch.mean(x_q, 1)\n",
        "#         x_q = self.relu(self.fc_q(x_q))\n",
        "\n",
        "#         #embedding layer\n",
        "#         x_p = passage * math.sqrt(self.args.hidden_size) #bs, seq_len_all, embedding_size\n",
        "#         x_p = self.dropout(x_p)\n",
        "\n",
        "#         #multi-atten\n",
        "#         x_p = torch.transpose(x_p, 0, 1) # seq_len, bs, embedding_size\n",
        "#         x = self.pos_encoder_p(x_p)\n",
        "#         x_p = self.multi_atten_p(x_p, src_key_padding_mask=(mask_p==0)) # seq_len, bs, embedding_size\n",
        "#         x_p = torch.transpose(x_p, 0, 1) #bs, seq_len, embedding_size\n",
        "        \n",
        "#         #pooling\n",
        "#         x_p = torch.mean(x_p, 1)\n",
        "#         x_p = self.relu(self.fc_p(x_p))\n",
        "\n",
        "#         x = x_q + x_p\n",
        "\n",
        "#         #down\n",
        "#         x = self.relu(self.down1(x))\n",
        "#         x = self.relu(self.down2(x))\n",
        "#         x = self.relu(self.down3(x))\n",
        "#         x = self.out(x)\n",
        "\n",
        "#         # x = self.out(x)\n",
        "\n",
        "#         x = nn.Sigmoid()(x)\n",
        "\n",
        "#         return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_ihb4uCGYIwY"
      },
      "outputs": [],
      "source": [
        "trainset = DataSeq(queries=train_queries, passages=train_passages, labels=train_labels)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=args.batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "valset = DataSeq(queries=test_queries, passages=test_passages, labels=test_labels)\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=args.batch_size, shuffle=False, num_workers=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqqPZbr3-yRD",
        "outputId": "9fcd760d-32ef-4973-b0ac-c8925eec8b4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Postive/Negative ratio in trainset is 0.0011\n"
          ]
        }
      ],
      "source": [
        "pos_ratio = np.where(train_labels==1)[0].shape[0] / train_labels.shape[0]\n",
        "print('Postive/Negative ratio in trainset is {0:.4f}'.format(pos_ratio))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ZSEl5cK5_NAC"
      },
      "outputs": [],
      "source": [
        "#Negative sampling\n",
        "import numpy as np\n",
        "def neg_sampling(qids, pids, labels, passages, queries, ratio=10):\n",
        "    ranked_qid = qids[np.argsort(qids)]\n",
        "    ranked_labels = labels[np.argsort(qids)]\n",
        "    ranked_passages = passages[np.argsort(qids)]\n",
        "    ranked_queries = queries[np.argsort(qids)]\n",
        "\n",
        "    last = ranked_qid[0]\n",
        "    counter = []\n",
        "    cur_counter = []\n",
        "    for i, id in enumerate(tqdm(ranked_qid)):\n",
        "        cur = id\n",
        "        if cur != last:\n",
        "            counter.append(cur_counter)\n",
        "            cur_counter = [i]\n",
        "        else:\n",
        "            cur_counter.append(i)\n",
        "        last = cur\n",
        "    counter.append(cur_counter)\n",
        "\n",
        "    sample_passage = []\n",
        "    sample_query = []\n",
        "    sample_label = []\n",
        "    \n",
        "    for i, qid in enumerate(tqdm(np.unique(qids))):\n",
        "        idx1 = np.where(ranked_labels[counter[i]] == 1)\n",
        "        num_pos = idx1[0].shape[0]\n",
        "\n",
        "        #positive sample\n",
        "        sample_passage.extend(ranked_passages[counter[i]][idx1])\n",
        "        sample_query.extend(ranked_queries[counter[i]][idx1])\n",
        "        sample_label.extend(ranked_labels[counter[i]][idx1])\n",
        "\n",
        "        #negative sample\n",
        "        idx0 = np.delete(np.arange(len(counter[i])), idx1)\n",
        "        np.random.shuffle(idx0)\n",
        "        sample_passage.extend(ranked_passages[counter[i]][idx0[:ratio*num_pos]])\n",
        "        sample_query.extend(ranked_queries[counter[i]][idx0[:ratio*num_pos]])\n",
        "        sample_label.extend(ranked_labels[counter[i]][idx0[:ratio*num_pos]])\n",
        "    \n",
        "    return sample_query, sample_passage, sample_label\n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vyrw7op5D9Up",
        "outputId": "eec92ea6-200c-4b06-af53-76500b6f2105"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4364339/4364339 [00:02<00:00, 1936827.41it/s]\n",
            "100%|██████████| 4590/4590 [00:03<00:00, 1516.80it/s]\n"
          ]
        }
      ],
      "source": [
        "sample_query, sample_passage, sample_label = neg_sampling(train_qids, train_pids, train_labels, train_passages, train_queries, ratio=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKriNwN7OCfK",
        "outputId": "93e615cc-da1f-4cf9-800d-215bea52fcf3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Postive/Negative ratio in trainset is 0.1668\n"
          ]
        }
      ],
      "source": [
        "pos_ratio = sum(sample_label) / len(sample_label)\n",
        "print('Postive/Negative ratio in trainset is {0:.4f}'.format(pos_ratio))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "f35H1dD5N3-z"
      },
      "outputs": [],
      "source": [
        "trainset = DataSeq(queries=sample_query, passages=sample_passage, labels=sample_label)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=args.batch_size, shuffle=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8JA7xN3429P",
        "outputId": "5b3a2810-6639-4fc5-ae62-562211d1b991"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([128, 50]),\n",
              " torch.Size([128, 300]),\n",
              " torch.Size([128, 50]),\n",
              " torch.Size([128, 300]),\n",
              " torch.Size([128]))"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# for i in trainloader:\n",
        "#     break\n",
        "\n",
        "# i[0].last_hidden_state.squeeze().shape, i[1].last_hidden_state.squeeze().shape, i[2].shape, i[3].squeeze().shape, i[4].squeeze().shape\n",
        "\n",
        "for i in trainloader:\n",
        "    break\n",
        "\n",
        "i[0].squeeze().shape, i[1].squeeze().shape, i[2].squeeze().shape, i[3].squeeze().shape,  i[4].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTk1w32o5DUd",
        "outputId": "5da5ff19-4462-4f82-c9fe-5aecffe599e3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch: 0: 100%|██████████| 225/225 [00:20<00:00, 10.76it/s, epoch_loss=0.500]\n",
            "Epoch: 1: 100%|██████████| 225/225 [00:20<00:00, 10.72it/s, epoch_loss=0.449]\n",
            "Epoch: 2: 100%|██████████| 225/225 [00:21<00:00, 10.49it/s, epoch_loss=0.442]\n",
            "Epoch: 3: 100%|██████████| 225/225 [00:21<00:00, 10.55it/s, epoch_loss=0.438]\n",
            "Epoch: 4: 100%|██████████| 225/225 [00:20<00:00, 10.73it/s, epoch_loss=0.434]\n",
            "Epoch: 5: 100%|██████████| 225/225 [00:21<00:00, 10.70it/s, epoch_loss=0.430]\n",
            "Epoch: 6: 100%|██████████| 225/225 [00:21<00:00, 10.68it/s, epoch_loss=0.428]\n",
            "Epoch: 7: 100%|██████████| 225/225 [00:20<00:00, 10.74it/s, epoch_loss=0.424]\n",
            "Epoch: 8: 100%|██████████| 225/225 [00:20<00:00, 10.72it/s, epoch_loss=0.422]\n",
            "Epoch: 9: 100%|██████████| 225/225 [00:20<00:00, 10.74it/s, epoch_loss=0.419]\n",
            "Epoch: 10: 100%|██████████| 225/225 [00:21<00:00, 10.69it/s, epoch_loss=0.415]\n",
            "Epoch: 11: 100%|██████████| 225/225 [00:21<00:00, 10.71it/s, epoch_loss=0.413]\n",
            "Epoch: 12: 100%|██████████| 225/225 [00:20<00:00, 10.74it/s, epoch_loss=0.409]\n",
            "Epoch: 13: 100%|██████████| 225/225 [00:20<00:00, 10.74it/s, epoch_loss=0.405]\n",
            "Epoch: 14: 100%|██████████| 225/225 [00:20<00:00, 10.73it/s, epoch_loss=0.401]\n",
            "Epoch: 15: 100%|██████████| 225/225 [00:20<00:00, 10.75it/s, epoch_loss=0.398]\n",
            "Epoch: 16: 100%|██████████| 225/225 [00:20<00:00, 10.76it/s, epoch_loss=0.394]\n",
            "Epoch: 17: 100%|██████████| 225/225 [00:20<00:00, 10.73it/s, epoch_loss=0.389]\n",
            "Epoch: 18: 100%|██████████| 225/225 [00:20<00:00, 10.72it/s, epoch_loss=0.384]\n",
            "Epoch: 19: 100%|██████████| 225/225 [00:20<00:00, 10.72it/s, epoch_loss=0.382]\n",
            "Epoch: 20: 100%|██████████| 225/225 [00:21<00:00, 10.70it/s, epoch_loss=0.378]\n",
            "Epoch: 21: 100%|██████████| 225/225 [00:20<00:00, 10.75it/s, epoch_loss=0.375]\n",
            "Epoch: 22: 100%|██████████| 225/225 [00:20<00:00, 10.77it/s, epoch_loss=0.370]\n",
            "Epoch: 23: 100%|██████████| 225/225 [00:20<00:00, 10.74it/s, epoch_loss=0.364]\n",
            "Epoch: 24: 100%|██████████| 225/225 [00:20<00:00, 10.75it/s, epoch_loss=0.361]\n",
            "Epoch: 25: 100%|██████████| 225/225 [00:20<00:00, 10.74it/s, epoch_loss=0.357]\n",
            "Epoch: 26: 100%|██████████| 225/225 [00:20<00:00, 10.72it/s, epoch_loss=0.353]\n",
            "Epoch: 27: 100%|██████████| 225/225 [00:20<00:00, 10.74it/s, epoch_loss=0.347]\n",
            "Epoch: 28: 100%|██████████| 225/225 [00:21<00:00, 10.70it/s, epoch_loss=0.343]\n",
            "Epoch: 29: 100%|██████████| 225/225 [00:20<00:00, 10.72it/s, epoch_loss=0.338]\n",
            "Epoch: 30: 100%|██████████| 225/225 [00:20<00:00, 10.73it/s, epoch_loss=0.334]\n",
            "Epoch: 31: 100%|██████████| 225/225 [00:20<00:00, 10.72it/s, epoch_loss=0.330]\n",
            "Epoch: 32: 100%|██████████| 225/225 [00:20<00:00, 10.76it/s, epoch_loss=0.327]\n",
            "Epoch: 33: 100%|██████████| 225/225 [00:20<00:00, 10.77it/s, epoch_loss=0.322]\n",
            "Epoch: 34: 100%|██████████| 225/225 [00:21<00:00, 10.69it/s, epoch_loss=0.316]\n",
            "Epoch: 35: 100%|██████████| 225/225 [00:21<00:00, 10.70it/s, epoch_loss=0.312]\n",
            "Epoch: 36: 100%|██████████| 225/225 [00:20<00:00, 10.75it/s, epoch_loss=0.306]\n",
            "Epoch: 37: 100%|██████████| 225/225 [00:20<00:00, 10.75it/s, epoch_loss=0.303]\n",
            "Epoch: 38: 100%|██████████| 225/225 [00:20<00:00, 10.72it/s, epoch_loss=0.301]\n",
            "Epoch: 39: 100%|██████████| 225/225 [00:21<00:00, 10.71it/s, epoch_loss=0.294]\n",
            "Epoch: 40: 100%|██████████| 225/225 [00:20<00:00, 10.72it/s, epoch_loss=0.290]\n",
            "Epoch: 41: 100%|██████████| 225/225 [00:20<00:00, 10.74it/s, epoch_loss=0.283]\n",
            "Epoch: 42: 100%|██████████| 225/225 [00:21<00:00, 10.70it/s, epoch_loss=0.282]\n",
            "Epoch: 43: 100%|██████████| 225/225 [00:20<00:00, 10.72it/s, epoch_loss=0.276]\n",
            "Epoch: 44: 100%|██████████| 225/225 [00:20<00:00, 10.74it/s, epoch_loss=0.275]\n",
            "Epoch: 45: 100%|██████████| 225/225 [00:20<00:00, 10.72it/s, epoch_loss=0.267]\n",
            "Epoch: 46: 100%|██████████| 225/225 [00:20<00:00, 10.74it/s, epoch_loss=0.262]\n",
            "Epoch: 47: 100%|██████████| 225/225 [00:21<00:00, 10.69it/s, epoch_loss=0.259]\n",
            "Epoch: 48: 100%|██████████| 225/225 [00:21<00:00, 10.61it/s, epoch_loss=0.255]\n",
            "Epoch: 49: 100%|██████████| 225/225 [00:21<00:00, 10.47it/s, epoch_loss=0.251]\n",
            "Epoch: 50: 100%|██████████| 225/225 [00:21<00:00, 10.64it/s, epoch_loss=0.248]\n",
            "Epoch: 51: 100%|██████████| 225/225 [00:21<00:00, 10.52it/s, epoch_loss=0.240]\n",
            "Epoch: 52: 100%|██████████| 225/225 [00:21<00:00, 10.33it/s, epoch_loss=0.243]\n",
            "Epoch: 53: 100%|██████████| 225/225 [00:21<00:00, 10.60it/s, epoch_loss=0.235]\n",
            "Epoch: 54: 100%|██████████| 225/225 [00:21<00:00, 10.71it/s, epoch_loss=0.232]\n",
            "Epoch: 55: 100%|██████████| 225/225 [00:21<00:00, 10.48it/s, epoch_loss=0.226]\n",
            "Epoch: 56: 100%|██████████| 225/225 [00:21<00:00, 10.70it/s, epoch_loss=0.223]\n",
            "Epoch: 57: 100%|██████████| 225/225 [00:21<00:00, 10.71it/s, epoch_loss=0.218]\n",
            "Epoch: 58: 100%|██████████| 225/225 [00:20<00:00, 10.73it/s, epoch_loss=0.217]\n",
            "Epoch: 59: 100%|██████████| 225/225 [00:21<00:00, 10.64it/s, epoch_loss=0.211]\n",
            "Epoch: 60: 100%|██████████| 225/225 [00:21<00:00, 10.70it/s, epoch_loss=0.205]\n",
            "Epoch: 61: 100%|██████████| 225/225 [00:20<00:00, 10.72it/s, epoch_loss=0.203]\n",
            "Epoch: 62: 100%|██████████| 225/225 [00:21<00:00, 10.70it/s, epoch_loss=0.203]\n",
            "Epoch: 63: 100%|██████████| 225/225 [00:20<00:00, 10.73it/s, epoch_loss=0.195]\n",
            "Epoch: 64: 100%|██████████| 225/225 [00:21<00:00, 10.66it/s, epoch_loss=0.192]\n",
            "Epoch: 65: 100%|██████████| 225/225 [00:20<00:00, 10.72it/s, epoch_loss=0.189]\n",
            "Epoch: 66: 100%|██████████| 225/225 [00:21<00:00, 10.70it/s, epoch_loss=0.184]\n",
            "Epoch: 67: 100%|██████████| 225/225 [00:21<00:00, 10.71it/s, epoch_loss=0.180]\n",
            "Epoch: 68: 100%|██████████| 225/225 [00:20<00:00, 10.72it/s, epoch_loss=0.181]\n",
            "Epoch: 69: 100%|██████████| 225/225 [00:21<00:00, 10.71it/s, epoch_loss=0.175]\n",
            "Epoch: 70: 100%|██████████| 225/225 [00:21<00:00, 10.71it/s, epoch_loss=0.169]\n",
            "Epoch: 71: 100%|██████████| 225/225 [00:21<00:00, 10.70it/s, epoch_loss=0.167]\n",
            "Epoch: 72: 100%|██████████| 225/225 [00:20<00:00, 10.72it/s, epoch_loss=0.165]\n",
            "Epoch: 73: 100%|██████████| 225/225 [00:21<00:00, 10.71it/s, epoch_loss=0.164]\n",
            "Epoch: 74: 100%|██████████| 225/225 [00:20<00:00, 10.72it/s, epoch_loss=0.161]\n",
            "Epoch: 75: 100%|██████████| 225/225 [00:20<00:00, 10.73it/s, epoch_loss=0.155]\n",
            "Epoch: 76: 100%|██████████| 225/225 [00:20<00:00, 10.73it/s, epoch_loss=0.150]\n",
            "Epoch: 77: 100%|██████████| 225/225 [00:20<00:00, 10.73it/s, epoch_loss=0.150]\n",
            "Epoch: 78: 100%|██████████| 225/225 [00:20<00:00, 10.75it/s, epoch_loss=0.144]\n",
            "Epoch: 79: 100%|██████████| 225/225 [00:21<00:00, 10.66it/s, epoch_loss=0.146]\n",
            "Epoch: 80: 100%|██████████| 225/225 [00:21<00:00, 10.71it/s, epoch_loss=0.145]\n",
            "Epoch: 81: 100%|██████████| 225/225 [00:21<00:00, 10.70it/s, epoch_loss=0.140]\n",
            "Epoch: 82: 100%|██████████| 225/225 [00:21<00:00, 10.71it/s, epoch_loss=0.134]\n",
            "Epoch: 83: 100%|██████████| 225/225 [00:20<00:00, 10.73it/s, epoch_loss=0.135]\n",
            "Epoch: 84: 100%|██████████| 225/225 [00:21<00:00, 10.69it/s, epoch_loss=0.125]\n",
            "Epoch: 85: 100%|██████████| 225/225 [00:20<00:00, 10.75it/s, epoch_loss=0.130]\n",
            "Epoch: 86: 100%|██████████| 225/225 [00:20<00:00, 10.73it/s, epoch_loss=0.131]\n",
            "Epoch: 87: 100%|██████████| 225/225 [00:20<00:00, 10.74it/s, epoch_loss=0.124]\n",
            "Epoch: 88: 100%|██████████| 225/225 [00:20<00:00, 10.72it/s, epoch_loss=0.122]\n",
            "Epoch: 89: 100%|██████████| 225/225 [00:21<00:00, 10.69it/s, epoch_loss=0.119]\n",
            "Epoch: 90: 100%|██████████| 225/225 [00:21<00:00, 10.70it/s, epoch_loss=0.114]\n",
            "Epoch: 91: 100%|██████████| 225/225 [00:21<00:00, 10.71it/s, epoch_loss=0.117]\n",
            "Epoch: 92: 100%|██████████| 225/225 [00:20<00:00, 10.73it/s, epoch_loss=0.115]\n",
            "Epoch: 93: 100%|██████████| 225/225 [00:21<00:00, 10.70it/s, epoch_loss=0.111]\n",
            "Epoch: 94: 100%|██████████| 225/225 [00:21<00:00, 10.67it/s, epoch_loss=0.107]\n",
            "Epoch: 95: 100%|██████████| 225/225 [00:20<00:00, 10.73it/s, epoch_loss=0.105]\n",
            "Epoch: 96: 100%|██████████| 225/225 [00:20<00:00, 10.74it/s, epoch_loss=0.102]\n",
            "Epoch: 97: 100%|██████████| 225/225 [00:21<00:00, 10.68it/s, epoch_loss=0.104]\n",
            "Epoch: 98: 100%|██████████| 225/225 [00:20<00:00, 10.73it/s, epoch_loss=0.103]\n",
            "Epoch: 99: 100%|██████████| 225/225 [00:21<00:00, 10.67it/s, epoch_loss=0.096]\n",
            "Epoch: 100: 100%|██████████| 225/225 [00:21<00:00, 10.70it/s, epoch_loss=0.099]\n",
            "Epoch: 101: 100%|██████████| 225/225 [00:20<00:00, 10.72it/s, epoch_loss=0.098]\n",
            "Epoch: 102: 100%|██████████| 225/225 [00:20<00:00, 10.73it/s, epoch_loss=0.094]\n",
            "Epoch: 103: 100%|██████████| 225/225 [00:21<00:00, 10.70it/s, epoch_loss=0.098]\n",
            "Epoch: 104: 100%|██████████| 225/225 [00:20<00:00, 10.73it/s, epoch_loss=0.087]\n",
            "Epoch: 105: 100%|██████████| 225/225 [00:20<00:00, 10.72it/s, epoch_loss=0.088]\n",
            "Epoch: 106: 100%|██████████| 225/225 [00:21<00:00, 10.69it/s, epoch_loss=0.087]\n",
            "Epoch: 107: 100%|██████████| 225/225 [00:21<00:00, 10.66it/s, epoch_loss=0.089]\n",
            "Epoch: 108: 100%|██████████| 225/225 [00:21<00:00, 10.69it/s, epoch_loss=0.082]\n",
            "Epoch: 109: 100%|██████████| 225/225 [00:21<00:00, 10.69it/s, epoch_loss=0.079]\n",
            "Epoch: 110: 100%|██████████| 225/225 [00:20<00:00, 10.72it/s, epoch_loss=0.082]\n",
            "Epoch: 111: 100%|██████████| 225/225 [00:21<00:00, 10.68it/s, epoch_loss=0.081]\n",
            "Epoch: 112: 100%|██████████| 225/225 [00:21<00:00, 10.70it/s, epoch_loss=0.080]\n",
            "Epoch: 113: 100%|██████████| 225/225 [00:21<00:00, 10.70it/s, epoch_loss=0.083]\n",
            "Epoch: 114: 100%|██████████| 225/225 [00:21<00:00, 10.70it/s, epoch_loss=0.079]\n",
            "Epoch: 115: 100%|██████████| 225/225 [00:21<00:00, 10.71it/s, epoch_loss=0.074]\n",
            "Epoch: 116: 100%|██████████| 225/225 [00:20<00:00, 10.73it/s, epoch_loss=0.072]\n",
            "Epoch: 117: 100%|██████████| 225/225 [00:20<00:00, 10.75it/s, epoch_loss=0.070]\n",
            "Epoch: 118: 100%|██████████| 225/225 [00:21<00:00, 10.69it/s, epoch_loss=0.075]\n",
            "Epoch: 119: 100%|██████████| 225/225 [00:20<00:00, 10.73it/s, epoch_loss=0.069]\n",
            "Epoch: 120: 100%|██████████| 225/225 [00:20<00:00, 10.73it/s, epoch_loss=0.068]\n",
            "Epoch: 121: 100%|██████████| 225/225 [00:21<00:00, 10.70it/s, epoch_loss=0.065]\n",
            "Epoch: 122: 100%|██████████| 225/225 [00:20<00:00, 10.72it/s, epoch_loss=0.071]\n",
            "Epoch: 123: 100%|██████████| 225/225 [00:21<00:00, 10.68it/s, epoch_loss=0.064]\n",
            "Epoch: 124: 100%|██████████| 225/225 [00:20<00:00, 10.72it/s, epoch_loss=0.066]\n",
            "Epoch: 125: 100%|██████████| 225/225 [00:20<00:00, 10.72it/s, epoch_loss=0.061]\n",
            "Epoch: 126: 100%|██████████| 225/225 [00:20<00:00, 10.75it/s, epoch_loss=0.064]\n",
            "Epoch: 127: 100%|██████████| 225/225 [00:20<00:00, 10.73it/s, epoch_loss=0.062]\n",
            "Epoch: 128: 100%|██████████| 225/225 [00:20<00:00, 10.72it/s, epoch_loss=0.063]\n",
            "Epoch: 129: 100%|██████████| 225/225 [00:21<00:00, 10.70it/s, epoch_loss=0.060]\n",
            "Epoch: 130: 100%|██████████| 225/225 [00:20<00:00, 10.74it/s, epoch_loss=0.060]\n",
            "Epoch: 131: 100%|██████████| 225/225 [00:21<00:00, 10.71it/s, epoch_loss=0.057]\n",
            "Epoch: 132: 100%|██████████| 225/225 [00:20<00:00, 10.74it/s, epoch_loss=0.061]\n",
            "Epoch: 133: 100%|██████████| 225/225 [00:20<00:00, 10.75it/s, epoch_loss=0.054]\n",
            "Epoch: 134: 100%|██████████| 225/225 [00:21<00:00, 10.69it/s, epoch_loss=0.058]\n",
            "Epoch: 135: 100%|██████████| 225/225 [00:21<00:00, 10.70it/s, epoch_loss=0.053]\n",
            "Epoch: 136: 100%|██████████| 225/225 [00:20<00:00, 10.74it/s, epoch_loss=0.052]\n",
            "Epoch: 137: 100%|██████████| 225/225 [00:21<00:00, 10.69it/s, epoch_loss=0.050]\n",
            "Epoch: 138: 100%|██████████| 225/225 [00:21<00:00, 10.65it/s, epoch_loss=0.056]\n",
            "Epoch: 139: 100%|██████████| 225/225 [00:21<00:00, 10.69it/s, epoch_loss=0.053]\n",
            "Epoch: 140: 100%|██████████| 225/225 [00:21<00:00, 10.68it/s, epoch_loss=0.050]\n",
            "Epoch: 141: 100%|██████████| 225/225 [00:21<00:00, 10.70it/s, epoch_loss=0.049]\n",
            "Epoch: 142: 100%|██████████| 225/225 [00:20<00:00, 10.73it/s, epoch_loss=0.050]\n",
            "Epoch: 143: 100%|██████████| 225/225 [00:21<00:00, 10.68it/s, epoch_loss=0.050]\n",
            "Epoch: 144: 100%|██████████| 225/225 [00:21<00:00, 10.70it/s, epoch_loss=0.050]\n",
            "Epoch: 145: 100%|██████████| 225/225 [00:21<00:00, 10.67it/s, epoch_loss=0.052]\n",
            "Epoch: 146: 100%|██████████| 225/225 [00:21<00:00, 10.69it/s, epoch_loss=0.054]\n",
            "Epoch: 147: 100%|██████████| 225/225 [00:21<00:00, 10.70it/s, epoch_loss=0.045]\n",
            "Epoch: 148: 100%|██████████| 225/225 [00:21<00:00, 10.67it/s, epoch_loss=0.049]\n",
            "Epoch: 149: 100%|██████████| 225/225 [00:20<00:00, 10.72it/s, epoch_loss=0.050]\n"
          ]
        }
      ],
      "source": [
        "binary_model = BinaryClassifier(args).to(args.device)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(binary_model.parameters(), lr=args.lr)\n",
        "# optimizer = torch.optim.SGD(binary_model.parameters(), lr=args.lr, momentum=0.9)\n",
        "\n",
        "\n",
        "for epoch in range(150):\n",
        "    progress_bar = tqdm(trainloader)\n",
        "    epoch_loss = 0.\n",
        "    for i, data in enumerate(progress_bar):\n",
        "        progress_bar.set_description('Epoch: ' + str(epoch))\n",
        "        # query, passage, query_mask, passage_mask, label = data[0].last_hidden_state.squeeze(), data[1].last_hidden_state.squeeze(), data[2].squeeze(), data[3].squeeze(), data[4].to(device)\n",
        "        query, passage, query_mask, passage_mask, label= data[0].squeeze().to(device), data[1].squeeze().to(device), data[2].squeeze().to(device), data[3].squeeze().to(device), data[4].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            query = model(input_ids=query, attention_mask=query_mask).last_hidden_state\n",
        "            passage = model(input_ids=passage, attention_mask=passage_mask).last_hidden_state\n",
        "        \n",
        "        output = binary_model(query, passage, query_mask, passage_mask)\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(output.squeeze(), label.to(torch.float32))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        progress_bar.set_postfix(epoch_loss='%.3f' % (epoch_loss /(i+1)))\n",
        "    # scheduler.step()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAfggrfSsWdD",
        "outputId": "d8d7a910-7a03-40c5-bb90-1e0b89aa4901"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BinaryClassifier(\n",
              "  (pos_encoder): PositionalEncoding(\n",
              "    (embeddings): Embedding(350, 128)\n",
              "  )\n",
              "  (multi_atten): TransformerEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0): TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=128, out_features=64, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear2): Linear(in_features=64, out_features=128, bias=True)\n",
              "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (1): TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=128, out_features=64, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear2): Linear(in_features=64, out_features=128, bias=True)\n",
              "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (fc): Linear(in_features=128, out_features=128, bias=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (down1): Linear(in_features=128, out_features=32, bias=True)\n",
              "  (down2): Linear(in_features=32, out_features=32, bias=True)\n",
              "  (down3): Linear(in_features=32, out_features=16, bias=True)\n",
              "  (out): Linear(in_features=16, out_features=1, bias=True)\n",
              "  (relu): ReLU()\n",
              ")"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# binary_model.eval()\n",
        "# torch.save(binary_model, '/content/gdrive/MyDrive/model.pth')\n",
        "binary_model = torch.load('model.pth')\n",
        "binary_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "rpk9r5rKq_uM"
      },
      "outputs": [],
      "source": [
        "test_queries = test_queries[np.argsort(test_qids)]\n",
        "test_passages = test_passages[np.argsort(test_qids)]\n",
        "test_labels = test_labels[np.argsort(test_qids)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "SJJ2leKErRRO"
      },
      "outputs": [],
      "source": [
        "valset = DataSeq(queries=test_queries, passages=test_passages, labels=test_labels)\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=512, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "QaOx6GgUjW8x"
      },
      "outputs": [],
      "source": [
        "# model.eval()\n",
        "# torch.save(model, '/content/gdrive/MyDrive/model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYdtBShFrSxM",
        "outputId": "b393a773-7a7e-4ae0-857b-f1051475c0dd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch: 0: 100%|██████████| 2155/2155 [06:31<00:00,  5.51it/s]\n"
          ]
        }
      ],
      "source": [
        "yTe_pred = []\n",
        "with torch.no_grad():\n",
        "    binary_model.eval()\n",
        "    progress_bar = tqdm(valloader)\n",
        "    for i, data in enumerate(progress_bar):\n",
        "        progress_bar.set_description('Epoch: ' + str(0))\n",
        "        query, passage, query_mask, passage_mask, label= data[0].squeeze().to(device), data[1].squeeze().to(device), data[2].squeeze().to(device), data[3].squeeze().to(device), data[4].to(device)\n",
        "        query = model(input_ids=query, attention_mask=query_mask).last_hidden_state\n",
        "        passage = model(input_ids=passage, attention_mask=passage_mask).last_hidden_state\n",
        "        output = binary_model(query, passage, query_mask, passage_mask)\n",
        "        yTe_pred.extend((output.squeeze().cpu().numpy().tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "5VC_qC0gXivq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def Average_Precision(true_label):\n",
        "    # AP for single query\n",
        "    rela_idx = np.where(true_label == 1)[0]\n",
        "    n_rela_passage = len(rela_idx)\n",
        "    denom = rela_idx + 1\n",
        "    numerator = np.arange(1, n_rela_passage+1)\n",
        "    return (numerator/denom/n_rela_passage).sum()\n",
        "\n",
        "def NDCG(true_label):\n",
        "    # NDCG for single query\n",
        "    DCG = np.sum((2**true_label - 1) / np.log2(np.arange(1, len(true_label)+1) + 1))\n",
        "    n_rela_passage = int(np.sum(true_label))\n",
        "    opt_rela_score = np.zeros(len(true_label))\n",
        "    opt_rela_score[:n_rela_passage] = 1\n",
        "    optDCG = np.sum((2**opt_rela_score - 1) / np.log2(np.arange(1, len(true_label)+1) + 1))\n",
        "    return DCG/optDCG.sum() if optDCG != 0 else 0\n",
        "    \n",
        "def mean_metric_nn(test_qids, test_pids, yTe, pred_yTe, write=True):\n",
        "    ranked_qid = test_qids[np.argsort(test_qids)]\n",
        "    last = ranked_qid[0]\n",
        "    counter = []\n",
        "    cur_counter = []\n",
        "    for i, id in enumerate(tqdm(ranked_qid)):\n",
        "        cur = id\n",
        "        if cur != last:\n",
        "            counter.append(cur_counter)\n",
        "            cur_counter = [i]\n",
        "        else:\n",
        "            cur_counter.append(i)\n",
        "        last = cur\n",
        "    counter.append(cur_counter)\n",
        "\n",
        "\n",
        "    m_ap = 0\n",
        "    m_ndcg = 0\n",
        "    test_pids = test_pids[np.argsort(test_qids)]\n",
        "    uni_qids = np.unique(test_qids)\n",
        "\n",
        "    res_qid = []\n",
        "    res_pid = []\n",
        "    res_score = []\n",
        "    res_rank = []\n",
        "    res_A1 = []\n",
        "    res_algoname = []\n",
        "\n",
        "    for i, count in enumerate(tqdm(counter)):\n",
        "        sort_idx = np.argsort(pred_yTe[count])[::-1]\n",
        "        sub_yTe = yTe[count]\n",
        "        true_label = sub_yTe[sort_idx]\n",
        "        ap = Average_Precision(true_label)\n",
        "        ndcg = NDCG(true_label)\n",
        "        m_ap += ap\n",
        "        m_ndcg += ndcg\n",
        "\n",
        "        sub_pids = test_pids[count]\n",
        "\n",
        "        res_qid.extend([uni_qids[i] for _ in range(len(sort_idx))])\n",
        "        res_pid.extend(sub_pids[sort_idx])\n",
        "        res_score.extend(pred_yTe[count][np.argsort(pred_yTe[count])[::-1]])\n",
        "        res_rank.extend(np.arange(1, len(sort_idx)+1))\n",
        "        res_A1.extend(['A1' for _ in range(len(sort_idx))])\n",
        "        res_algoname.extend(['NN' for _ in range(len(sort_idx))])\n",
        "\n",
        "\n",
        "    if write:\n",
        "        data = {'qid': res_qid, 'A1': res_A1, 'pid': res_pid, 'rank': res_rank, 'score': res_score, 'algoname': res_algoname}\n",
        "        data_df = pd.DataFrame(data)\n",
        "        data_df.to_csv('NN.txt',index=False,header=False, sep=' ')\n",
        "    \n",
        "        \n",
        "    \n",
        "    return m_ap / len(counter), m_ndcg / len(counter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Je39Cx1Vwq_W",
        "outputId": "12ce8a4b-c316-4c2a-e269-a54d96f2cb9c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1103039/1103039 [00:00<00:00, 2094042.37it/s]\n",
            "100%|██████████| 1148/1148 [00:01<00:00, 934.95it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.03623747755962718, 0.16870018637126288)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mean_metric_nn(test_qids, test_pids, test_labels, np.array(yTe_pred))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "oKesiBpAXIIS"
      ],
      "machine_shape": "hm",
      "name": "COMP0084_CW2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "011333a3073e42a8bde3278ac43d651a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "037aa7ec3f3547a2a5a58d322f7cc792": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "042198e1b2ed4a1091cd3bf46727ea92": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1509220e320742f98493c42700b6eafb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_037aa7ec3f3547a2a5a58d322f7cc792",
            "placeholder": "​",
            "style": "IPY_MODEL_88107ed83c4c416dad7fe250b793796c",
            "value": " 226k/226k [00:00&lt;00:00, 653kB/s]"
          }
        },
        "357221617b344cbca49f6169ce4ce79c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a895929ba5604444b6faa39b9385d408",
            "placeholder": "​",
            "style": "IPY_MODEL_042198e1b2ed4a1091cd3bf46727ea92",
            "value": "Downloading: 100%"
          }
        },
        "370faac1cdd748a2986f51be052d58ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_357221617b344cbca49f6169ce4ce79c",
              "IPY_MODEL_c4f48d1e7eb147c2bd0dda1e26f9518e",
              "IPY_MODEL_990cb7ebce91462db5846bd82f28be41"
            ],
            "layout": "IPY_MODEL_b4251e16e5de431fae645212cc6697ca"
          }
        },
        "4f3d7ddf0fa847268fd31f16cf0754a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bfffa9c87b1e4523bd11ba23d86c4fd2",
            "max": 17756393,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_011333a3073e42a8bde3278ac43d651a",
            "value": 17756393
          }
        },
        "51cb41f944d548b9a24601e93d7a341d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5524c6e0ddea4c6bbb8e1aaa8c290c93": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "55aa9404de50414f87d0f2064ff43c8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "56f5bdc1b5f0453d881dad1e80f72de4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5a83992adb304f76905ecea68d774eb9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5cd9f0e6a8d9402d81067c50c0f55ef7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8f93e2f54e2d49aa98842ea596ada03f",
              "IPY_MODEL_e7790a0a0b924ee2a2166f3779b04635",
              "IPY_MODEL_1509220e320742f98493c42700b6eafb"
            ],
            "layout": "IPY_MODEL_7d5a590ddd80427d9275941d173da78a"
          }
        },
        "5f6b5b4c4d9e4085b840f03ba57882be": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6506fbc049cd4c74a6f1bcfba4be4036": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "71f905022fa04a9fb2600951cf9d25d5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d5a590ddd80427d9275941d173da78a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84293a5e8aa74abfbd3425b974b43790": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86707d28c6cb4d759067e8c666215a97": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_96fd80053c1d4c49a07041bb812b25fa",
              "IPY_MODEL_4f3d7ddf0fa847268fd31f16cf0754a7",
              "IPY_MODEL_8a275414bc9c4e9eb3b0391e221b487e"
            ],
            "layout": "IPY_MODEL_9a73623a9bd24451ab8f23571a4a3004"
          }
        },
        "88107ed83c4c416dad7fe250b793796c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8a275414bc9c4e9eb3b0391e221b487e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f6b5b4c4d9e4085b840f03ba57882be",
            "placeholder": "​",
            "style": "IPY_MODEL_55aa9404de50414f87d0f2064ff43c8e",
            "value": " 16.9M/16.9M [00:00&lt;00:00, 39.3MB/s]"
          }
        },
        "8f93e2f54e2d49aa98842ea596ada03f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71f905022fa04a9fb2600951cf9d25d5",
            "placeholder": "​",
            "style": "IPY_MODEL_5524c6e0ddea4c6bbb8e1aaa8c290c93",
            "value": "Downloading: 100%"
          }
        },
        "96fd80053c1d4c49a07041bb812b25fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51cb41f944d548b9a24601e93d7a341d",
            "placeholder": "​",
            "style": "IPY_MODEL_6506fbc049cd4c74a6f1bcfba4be4036",
            "value": "Downloading: 100%"
          }
        },
        "990cb7ebce91462db5846bd82f28be41": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff6759a4b8334b11b4ce659f59f5d33b",
            "placeholder": "​",
            "style": "IPY_MODEL_56f5bdc1b5f0453d881dad1e80f72de4",
            "value": " 285/285 [00:00&lt;00:00, 9.40kB/s]"
          }
        },
        "9a73623a9bd24451ab8f23571a4a3004": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a58d0fe5fe814706b69c48f64dec9d9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a895929ba5604444b6faa39b9385d408": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4251e16e5de431fae645212cc6697ca": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bfffa9c87b1e4523bd11ba23d86c4fd2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4f48d1e7eb147c2bd0dda1e26f9518e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a83992adb304f76905ecea68d774eb9",
            "max": 285,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f070ac6788734936b68ee599eeb9d1fb",
            "value": 285
          }
        },
        "e7790a0a0b924ee2a2166f3779b04635": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_84293a5e8aa74abfbd3425b974b43790",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a58d0fe5fe814706b69c48f64dec9d9a",
            "value": 231508
          }
        },
        "f070ac6788734936b68ee599eeb9d1fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ff6759a4b8334b11b4ce659f59f5d33b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
